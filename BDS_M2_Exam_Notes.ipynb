{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXZ+Tb+YrcDmmKe11hQNQ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadiaHolmlund/BDS_M2_Exam_Notes/blob/main/BDS_M2_Exam_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Analysis"
      ],
      "metadata": {
        "id": "Ydr8ZL3hAI-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a network?"
      ],
      "metadata": {
        "id": "tocpWFQsBCQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A network is a system of elements (nodes/vertices) and connections (edges/links) between them. Networks are used to present relational data and can be applied to many types of relationships between different types of elements.\n",
        "\n",
        "\n",
        "nodes: system theory jargon\n",
        "\n",
        "vertices: graph theory jargon\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Unknown)"
      ],
      "metadata": {
        "id": "1XTz7eXPBFAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of networks"
      ],
      "metadata": {
        "id": "41fG2OLjCef9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The content, meaning and interpretation of networks depends of the elements and relationships displayed. Types of networks includes:\n",
        "\n",
        "Social networks:\n",
        "- Nodes/vertices represent actors (persons, firms, other socially constructed entitites)\n",
        "\n",
        "- Edges/links represent relationships between actors (friendship, interaction, co-affiliation, similarity, etc)\n",
        "\n",
        "Other networks:\n",
        "- Chemistry: Interaction between molecules\n",
        "- Computer Science: The world-wide-web, inter- and intranet topologies\n",
        "- Biology: Food-web, ant-hives"
      ],
      "metadata": {
        "id": "b0kyFInMDGTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The possibilities to depict relational data are manifold, e.g.:\n",
        "\n",
        "Relations among persons\n",
        "- Kinship: mother of, wife of…\n",
        "- Other role based: boss of, supervisor of…\n",
        "- Affective: likes, trusts…\n",
        "- Interaction: give advice, talks to, retweets…\n",
        "- Affiliation: belong to same clubs, shares same interests…\n",
        "\n",
        "Relations among organizations\n",
        "- As corporate entities, joint ventures, strategic alliances\n",
        "- Buy from / sell to, leases to, outsources to\n",
        "- Owns shares of, subsidiary of\n",
        "- Via their members (Personnel flows, friendship…)"
      ],
      "metadata": {
        "id": "jppmXsJMEz0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relational data structures"
      ],
      "metadata": {
        "id": "bbZ0UBojFFGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Edgelist"
      ],
      "metadata": {
        "id": "AJZzWF2uKIz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A common form of storing relational data\n",
        "- An edgelist is a dataframe containing minimum two columns:\n",
        "  - column 1: Source node of a connection\n",
        "  - column 2: Target node of a connection\n",
        "- Nodes are typically identified by unique IDs\n",
        "- An edge list can also contain additional columns that describe **attributes** of the edges such as magnitude aspects for an edge. If the edges have a magnitude attribute the graph is considered **weighted** (e.g., number of interactions, strenght of friendship). \n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/maxresdefault.jpg)\n"
      ],
      "metadata": {
        "id": "SfndnuXOFG-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjacency matrix / Socio matrix"
      ],
      "metadata": {
        "id": "UnIn_-psKMXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Represented as a n*n matrix, where n stands for the number of elements (nodes/vertices) of which relationships should be represented\n",
        "- The value in the cell that intercepts row n and column m indicates if an edge is present (=1) or absent (=0).\n",
        "- An adjacency matrix can be produced by crosstabulating an edgelist\n",
        "\n",
        "Note the impact of directed vs. undirected vs. weighted networks on adjacency matrices\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Different-types-of-graphs-and-their-corresponding-adjacency-matrix-representations-The.ppm.png)"
      ],
      "metadata": {
        "id": "KbVA1BdfHr_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nodelist"
      ],
      "metadata": {
        "id": "jDX0Vbr3KQak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Contains information about the nodes, aka attributes (edgelist and adjacency matrix stores only connectivity patterns ***between*** nodes)\n",
        "  - e.g. name, gender, age, group etc.\n",
        "\n",
        "  ![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.00.45.png)"
      ],
      "metadata": {
        "id": "3VF4g8ioKR44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network graphs"
      ],
      "metadata": {
        "id": "R_5wYXeHMT09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph objects"
      ],
      "metadata": {
        "id": "WXvYfAgcMVRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tabular data dependency\n",
        "- Between observation dependency: Summary statistics of variables are between observations (column-wise) interdependent, meaning changing a value of some observation will change the corresponding variables summary statistics.\n",
        "- Within observation dependency: Summary statitics of variables are within observations (row-wise) interdependent, meaning changing a variable value might change summary statistics of the observation\n",
        "- Otherwise, values are (at least mathematically) independent"
      ],
      "metadata": {
        "id": "3v0C_DWiOwXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph data dependency\n",
        "- Above holds true, but graph data holds additional dependencies due to the relational structure of data.\n",
        "- E.g. adding/removing node(s) may imply adding/removing edge(s) and adding/removing edge(s) may change the characteristics of node(s), due to their relational interdependence\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.24.25.png)\n"
      ],
      "metadata": {
        "id": "HfKysRcZQChy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph concepts and terminology"
      ],
      "metadata": {
        "id": "Wj20Vib3MXeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The vertices ***u*** and ***v*** are called the end vertices of the edge ***(u,v)***\n",
        "- If two edges have the same end vertices they are ***Parallel***\n",
        "- An edge of the form ***(v,v)*** is a ***loop***\n",
        "- A Graph is ***simple*** if it has no parallel edges and loops\n",
        "- A Graph is said to be ***Empty*** if it has no edges. Meaning ***E*** is empty\n",
        "- A Graph is a ***Null Graph*** if it has no vertices. Meaning ***V*** and ***E*** is empty\n",
        "- Edges are ***Adjacent*** if they have a common vertex. Vertices are ***Adjacent*** if they have a common edge\n",
        "- The ***degree*** of the vertex ***v***, written as ***d(v)***, is the number of edges with v as an end vertex. By convention, we count a loop twice and parallel edges contribute separately\n",
        "- ***Isolated*** Vertices are vertices with degree 1.\n",
        "- A Graph is ***Complete*** if its edge set contains every possible edge between ALL of the vertices\n",
        "- A ***Walk*** in a Graph ***G = (V,E)*** is a finite, alternating sequence of the form ViEiViEi consisting of vertices and edges of the graph ***G***\n",
        "- A ***Walk*** is ***Open*** if the initial and final vertices are different. A ***Walk*** is ***Closed*** if the initial and final vertices are the same"
      ],
      "metadata": {
        "id": "1mRS7q9OR1y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of graphs"
      ],
      "metadata": {
        "id": "eycSQC-mhw8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Weigthed vs. Unweighted\n",
        "2. Directed vs. Undirected\n",
        "3. Unimodal vs. Multimodal\n",
        "4. Unidimensional vs. Multidimensional\n",
        "\n",
        "`networkx` graph classes\n",
        "1. Graph\n",
        "2. DiGraph\n",
        "3. MultiGraph\n",
        "4. MultiDigraph"
      ],
      "metadata": {
        "id": "G9ShadZ6h009"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Weigthed vs. Unweighted"
      ],
      "metadata": {
        "id": "9np0YgUElKDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted: edges have values associated with them\n",
        "\n",
        "Unweighted: edges either exist or do not\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.48.07.png)"
      ],
      "metadata": {
        "id": "qKpZ7ORTgdEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Directed vs. Undirected"
      ],
      "metadata": {
        "id": "mg2w257VlNJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Directed: edges are not necessarily reciprocated\n",
        "\n",
        "Undirected: edges are always mutual\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.46.51.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.46.34.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.46.24.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.46.01.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.47.34.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.47.05.png)"
      ],
      "metadata": {
        "id": "3oFGtT4yfm2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unimodal vs. Multimodal"
      ],
      "metadata": {
        "id": "wYgsHp00lOlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unimodal networks (1-mode): include only one type of node (e.g. all nodes represent people)\n",
        "\n",
        "![](https://toreopsahl.files.wordpress.com/2009/04/fig1_twomode_simple.png)\n",
        "\n",
        "Multimodal (2-mode / Bipartite / Bimodal): include more than one type of node (.e.g people and research papers)\n",
        "\n",
        "![](https://toreopsahl.files.wordpress.com/2009/04/fig1_twomode_half.png)"
      ],
      "metadata": {
        "id": "u_4hJFTKgAH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unidimensional vs. Multidimensional"
      ],
      "metadata": {
        "id": "-8I_Q4NFlUz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unidimensional: includes one type of edges\n",
        "\n",
        "Multidimensional: Includes multiple types of edges (can be analysed as multiplex network or multiple distinct networks)\n",
        "\n",
        "Multidimensional networks are a special type of multilayer networks with multiple types of relations. They also consist of nodes and edges, but the nodes exist in separate layers, representing different forms of interactions, which connect to form an aspect. Aspects (or stacks of layers) can be used to represent different types of contacts, spatial locations, subsystems, or points in time\n",
        "\n",
        "Example:\n",
        "The multiplex social network of Star Wars saga. Each layer denotes a different episode and two nodes are connected to each other if the corresponding characters acted together in one or more scenes.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Muxviz_Star_Wars_Social_Network.png)"
      ],
      "metadata": {
        "id": "sjtZDb-CjItJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing networks"
      ],
      "metadata": {
        "id": "zHajRgMGhEq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Matrix plots"
      ],
      "metadata": {
        "id": "FsHCANjghJ9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.50.20.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.50.58.png)"
      ],
      "metadata": {
        "id": "b8ssFLOIhLuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Arc plots"
      ],
      "metadata": {
        "id": "IA4JS0bmhP20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.51.33.png)"
      ],
      "metadata": {
        "id": "mclxYej0hc8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Circos plots"
      ],
      "metadata": {
        "id": "9HgBCrAqhR7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.52.19.png)"
      ],
      "metadata": {
        "id": "tuaFUvFhhhmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Irrational vs. rational graphs"
      ],
      "metadata": {
        "id": "NVbel096g2tH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.49.42.png)"
      ],
      "metadata": {
        "id": "N8zfg0umg5Qe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NetworkX library"
      ],
      "metadata": {
        "id": "MAYJhQxWhSFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A graph object is a specific datastructure which contains node and edgelists jointly, and enables the application of graph algorithms on them. We work with the [`networkx`](https://networkx.github.io/documentation/stable/index.html) library, which is the standard for network analysis in the Python community."
      ],
      "metadata": {
        "id": "4RptyNgThUCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx # Main network analysis library"
      ],
      "metadata": {
        "id": "M6ILRfepxNU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In NetworkX, graph data are stored in a dictionary-like fashion.\n",
        "They are placed under a `Graph` object,\n",
        "canonically instantiated with the variable `G` as follows:\n",
        "\n",
        "```python\n",
        "G = nx.Graph()\n",
        "```\n",
        "\n",
        "Of course, you are free to name the graph anything you want!\n",
        "\n",
        "Nodes are part of the attribute `G.nodes`.\n",
        "There, the node data are housed in a dictionary-like container,\n",
        "where the key is the node itself\n",
        "and the values are a dictionary of attributes. \n",
        "Node data are accessible using syntax that looks like:\n",
        "\n",
        "```python\n",
        "G.nodes[node1]\n",
        "```\n",
        "\n",
        "Edges are part of the attribute `G.edges`,\n",
        "which is also stored in a dictionary-like container.\n",
        "Edge data are accessible using syntax that looks like: \n",
        "\n",
        "```python\n",
        "G.edges[node1, node2]\n",
        "```\n",
        "Because of the dictionary-like implementation of the graph,\n",
        "any hashable object can be a node.\n",
        "This means strings and tuples, but not lists and sets."
      ],
      "metadata": {
        "id": "aPCP1SrBRsIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local network structure (node-level measures)"
      ],
      "metadata": {
        "id": "arOixM0aUJxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methods to summarise the pattern of node connectivity to inter something on their characteristics.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.38.56.png)"
      ],
      "metadata": {
        "id": "cmW2pE-oUidz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Degree centrality"
      ],
      "metadata": {
        "id": "GkuGCdNAVB8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Counts the number of edges adjacent to a node.\n",
        "- Formally, the degree of node $i$ is the number of existing edges $e_{ij}$ with other nodes $j$ in a network with $n$ nodes:\n",
        "\n",
        "$$d_{ij} =\\sum\\limits_{j=1}^{n} e_{ij} ~ where: ~ i \\neq j$$\n",
        "\n",
        "**Degree centrality in directed networks**\n",
        "\n",
        "In directed networks, a node-pair has two different roles:\n",
        "\n",
        "* **Ego:** The node the edge originates from.\n",
        "* **Alter:** The node the edge leads to.\n",
        "\n",
        "Network metrics have to take directionality into account. For example, degree centrality is now differentiated between the\n",
        "- **in-degree** centrality (how many edges lead ***to*** the node)\n",
        "- **out-degree** centrality (how many edges lead ***from*** the node)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.43.47.png)"
      ],
      "metadata": {
        "id": "8ADMvslxVjvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eigenvector centrality"
      ],
      "metadata": {
        "id": "RHqmrFCFVXWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Weighs a node's degree centrality by the centrality of the nodes adjacent to it (and their centrality in turn by their centrality).\n",
        "\n",
        "$$x_{v}={\\frac {1}{\\lambda }}\\sum _{t\\in M(v)}x_{t}={\\frac {1}{\\lambda }}\\sum _{t\\in G}a_{v,t}x_{t}$$\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.48.08.png)"
      ],
      "metadata": {
        "id": "j360eHOeWLeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Betweenness centrality"
      ],
      "metadata": {
        "id": "oLp_3obCVc_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Measures the extent to which it lies on short paths.\n",
        "- A higher betweenness indicates that a node lies on more short paths and hence should somehow be important for traversing between different parts of a network.\n",
        "\n",
        "In formulaic representation\n",
        "\n",
        "* The geodesic betweenness $B_{n}(i)$ of a **vertex** in a weighted, undirected network is\n",
        "\n",
        "$$B_{n}(i) =  \\sum_{s,t \\in G} \\frac{ \\Psi_{s,t}(i) }{\\Psi_{s,t}}$$\n",
        "where vertices $s,t,i$ are all different from each other\n",
        "\n",
        "* $\\Psi_{s,t}$ denotes the number of shortest paths (geodesics) between vertices $s$ and $t$\n",
        "* $\\Psi_{s,t}(i)$ denotes the number of shortest paths (geodesics) between vertices $s$ and $t$ **that pass through vertex** $i$.\n",
        "* The geodesic betweenness $B_n$ of a network is the mean of $B_n(i)$ over all vertices $i$\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.51.47.png)"
      ],
      "metadata": {
        "id": "zjAhCzJFXSyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neighborhood"
      ],
      "metadata": {
        "id": "fGExOYbRYBDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Examines the surroundings of a node in terms of the nodes it is connected to, i.e. it's neighborhood\n",
        "- Ego-network of node: How many nodes are in a certain geodesic distance (meaning the shortest path), i.e. how many nodes are not more than x-steps away.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.57.16.png)"
      ],
      "metadata": {
        "id": "d0fHzek6YFwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modularity / Clustering (Community detection)\n",
        "What is within- and between network connectivity? Impossible to google...."
      ],
      "metadata": {
        "id": "op8IJNQoZISL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Group nodes based on graph topology (sometimes referred to as community detection based on its commonality in social network analysis)\n",
        "- Main logic: Form groups which have a ***maximum within-connectivity*** and a ***minimum between-connectivity***.\n",
        "- Consequently: Nodes in the same community should have a higher probability of being connected than nodes from different communities.\n",
        "\n",
        "**Community clustering in directed networks**\n",
        "\n",
        "Most community detection algorithms implemented in `NetworkX` only work with undirected networks. So, we can do 2 things to handle these:\n",
        "\n",
        "1. Convert the network in an undirected one.\n",
        "2. Use the \"edge betweenness\" algorithm, the only one implemented that can handle directed networks.\n",
        "\n",
        "There are (just like for clustering of tabular data in UML) many different algorithms and approaches to detect and delineate communities. [Here](https://github.com/benedekrozemberczki/awesome-community-detection) you find a summary of currently used approaches."
      ],
      "metadata": {
        "id": "sJBWbOoOZJkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: The Louvain Algorithm\n",
        "\n",
        "One of the most widely used community detection algorithms. It usually delivers good results, scales well, and can handle weighted networks. Furthermore, there is an actively maintained, easy to use Python implementation, [`python-louvain`](https://python-louvain.readthedocs.io).\n",
        "\n",
        "It optimises a quantity called modularity:\n",
        "\n",
        "$$  \\sum_{ij} (A_{ij} - \\lambda P_{ij}) \\delta(c_i,c_j) $$\n",
        "\n",
        "$A$ - The adjacency matrix\n",
        "\n",
        "$P_{ij}$ - The expected connection between $i$ and $j$.\n",
        "\n",
        "$\\lambda$ - Resolution parameter\n",
        "\n",
        "Can use lots of different forms for $P_{ij}$ but the standard one is the so called configuration model:\n",
        "\n",
        "$P_{ij} = \\frac{k_i k_j}{2m}$\n",
        "\n",
        "Loosely speaking, in an iterative process:\n",
        "- You take a node and try to aggregate it to one of its neighbours.\n",
        "- You choose the neighbour that maximizes a modularity function.\n",
        "- Once you iterate through all the nodes, you will have merged few nodes together and formed some communities.\n",
        "- This becomes the new input for the algorithm that will treat each community as a node and try to merge them together to create bigger communities.\n",
        "- The algorithm stops when it’s not possible to improve modularity any more.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2013.04.56.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2013.06.10.png)"
      ],
      "metadata": {
        "id": "E7OcvXIHaZRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assortiativity"
      ],
      "metadata": {
        "id": "N2qUHD_Fnrvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Measures if two nodes that share certain characteristics have a higher or lower probability to be connected.\n"
      ],
      "metadata": {
        "id": "WQgAKabQnuf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reciprocity"
      ],
      "metadata": {
        "id": "AOt2P7_an5Qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Measures if directed edges are reciptocated, meaning that an edge between `i,j` makes an edge between `j,i` more likely"
      ],
      "metadata": {
        "id": "HssUMeW1NO71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global network structure (overall-level measures)"
      ],
      "metadata": {
        "id": "VvEAQMZtbml2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Density"
      ],
      "metadata": {
        "id": "UL41dMJEb5jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The density of a measure represents the share of all possible connections in the network."
      ],
      "metadata": {
        "id": "whjcF-_1cL6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transitivity / Clustering Coefficient"
      ],
      "metadata": {
        "id": "dVNbkMPzb-UK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transitivity, also called the Clustering Cefficient indicates how much the network tends to be locally clustered. That is measured by the share of closed triplets."
      ],
      "metadata": {
        "id": "Az01DuuCccMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diameter"
      ],
      "metadata": {
        "id": "ZGStDpQ0cBGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The diameter is the longest of the shortest paths between two nodes of the network."
      ],
      "metadata": {
        "id": "4bIojY_MdzrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean distance / Average path lenght"
      ],
      "metadata": {
        "id": "8UTh6SU3cDgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The mean distance / average path lenght represents the mean of all shortest paths between all nodes. It is a measure of diffusion potential within a network."
      ],
      "metadata": {
        "id": "ejUylNTbd9CR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Small worlds"
      ],
      "metadata": {
        "id": "RYx5dKfykeyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small worlds are an interesting network structure, combining short path lenght betwen the nodes with a high clustering coefficient. That means, that we have small interconected clusters, which are in turn connected by **gatekeepers** (the edges we call **bridges** or **structural holes**). \n",
        "\n",
        "A small-world network is a type of mathematical graph in which most nodes are not neighbors of one another, but the neighbors of any given node are likely to be neighbors of each other and most nodes can be reached from every other node by a small number of hops or steps.\n",
        "\n",
        "Mathematically, small world networks of size n have an average distance O(log n), meaning that between any two random nodes, the expected distance is O(log n).\n",
        "\n",
        "⟨L⟩ ∝ log n\n",
        "\n",
        "Small-world network example\n",
        "Hubs are bigger than other nodes\n",
        "Average degree= 3.833\n",
        "Average shortest path length = 1.803.\n",
        "Clustering coefficient = 0.522\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Small-world-network-example.png)"
      ],
      "metadata": {
        "id": "xRRqgXqlkgZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity networks"
      ],
      "metadata": {
        "id": "3Kppch4Zoz_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simirality networks are constructed by mapping similarity between all observations, e.g. using\n",
        "- Cosine Similarity\n",
        "- Pearson Coefficient\n",
        "- Euclidean Distance"
      ],
      "metadata": {
        "id": "NRWnf-Ico1tx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Cosine distance"
      ],
      "metadata": {
        "id": "tWLI1e1yvgod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity takes into account the degree of the vertices or how many common neighbors other pairs of vertices has and also allow for varying degrees of vertices.\n",
        "\n",
        "Salton proposed that we regard the i-th and j-th rows/columns of the adjacency matrix as two vectors and use the cosine of the angle between them as a similarity measure. The cosine similarity of i and j is the number of common neighbors divided by the geometric mean of their degrees.\n",
        "\n",
        "Its value lies in the range from 0 to 1. The value of 1 indicates that the two vertices have exactly the same neighbors while the value of zero means that they do not have any common neighbors. Cosine similarity is technically undefined if one or both of the nodes has zero degree, but according to the convention, we say that cosine similarity is 0 in these cases."
      ],
      "metadata": {
        "id": "ARgFFz7xvets"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pearson Coefficent"
      ],
      "metadata": {
        "id": "fjKeKQWEv45p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson product-moment correlation coefficient is an alternative method to normalize the count of common neighbors. This method compares the number of common neighbors with the expected value that count would take in a network where vertices are connected randomly. This quantity lies strictly in the range from -1 to 1."
      ],
      "metadata": {
        "id": "sTthL09Hv6yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Euclidean distance"
      ],
      "metadata": {
        "id": "rtZE1r4wwLoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Euclidean distance is equal to the number of neighbors that differ between two vertices. It is rather a dissimilarity measure, since it is larger for vertices which differ more. It could be normalized by dividing by its maximum value. The maximum means that there are no common neighbors, in which case the distance is equal to the sum of the degrees of the vertices."
      ],
      "metadata": {
        "id": "6ouKoBp4wFsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multimodal network analysis"
      ],
      "metadata": {
        "id": "uwCRKTh2pLiZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IhBLLexCOPh"
      },
      "source": [
        "Multi-modal means a network has several \"modes\", i.e. it connects entities on different conceptual levels. The most common is a **2-mode** (or **bipartite**) network. \n",
        "\n",
        "Examples could be:\n",
        "\n",
        "* Author $\\rightarrow$ Paper\n",
        "* Inventor $\\rightarrow$ Patent\n",
        "* Member $\\rightarrow$ Club network. \n",
        "\n",
        "Here, elements in different modes represent different things. In real-life research examples you find 2-mode networks in for instance:\n",
        "- co-occurence (2 actors mentioned in the same news-article)\n",
        "- co-affiliation (2 actors are member of the same association)\n",
        "- co-characteristics (2 actors both like to talk about a certain topic on twitter)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network projection"
      ],
      "metadata": {
        "id": "TRrhE8wEuFIH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rVL3iyWCOmY"
      },
      "source": [
        "Two-mode networks are rarely analysed in their original form. Although this is preferable, few methods exist for that purpose. As such, these networks are often transformed into one-mode networks (only one type of nodes) to be analysed. This procedure is often referred to as projection. Projection is done by selecting one of the sets of nodes and linking two nodes from that set if they were connected to the same node (of the other kind).\n",
        "\n",
        "We can alalyse them in sepperation (and sometimes we should), but often its helpful to *project* them onto one mode. Here, we create a node in one mode by joint association with another mode.\n",
        "\n",
        "2-mode\n",
        "\n",
        "![](https://toreopsahl.files.wordpress.com/2009/04/fig1_twomode_half.png)\n",
        "\n",
        "1-mode\n",
        "\n",
        "![](https://toreopsahl.files.wordpress.com/2009/04/fig1_twomode_simple.png)\n",
        "\n",
        "![](https://www.dropbox.com/s/e4vnq7kh24pyu0t/networks_2mode.png?dl=1)\n",
        "\n",
        "Particularly in citation networks, we can also use the implicite 2-mode structure of $Publications \\rightarrow Citation$\n",
        "\n",
        "That helps us to apply some interesting metrics, such as:\n",
        "\n",
        "* direct citations\n",
        "* Bibliographic coupling\n",
        "* Co--citations\n",
        "\n",
        "Interestingly, different projections of this 2-mode network give the whole resulting 1-mode network a different meaning.\n",
        "\n",
        "![](https://www.dropbox.com/s/f8g8nr83lucvpqx/networks_biblio.png?dl=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf8inmy9WuL-"
      },
      "source": [
        "# Example: pull the edges from the networkx graph object and turn it into a pandas edgelist (dataframe)\n",
        "# pull edges\n",
        "edges_df = nx.to_pandas_edgelist(B)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQkPKA06xeQj",
        "outputId": "f36cffa3-8735-44e9-e542-342d0ae83b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "edges_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Christian</td>\n",
              "      <td>Crossfit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lise</td>\n",
              "      <td>Jomfru Anne Gade</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lise</td>\n",
              "      <td>Jazz Club</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Casper</td>\n",
              "      <td>Crossfit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Casper</td>\n",
              "      <td>Yoga House</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Casper</td>\n",
              "      <td>Jazz Club</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Helle</td>\n",
              "      <td>Yoga House</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Helle</td>\n",
              "      <td>Crossfit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Morten</td>\n",
              "      <td>Crossfit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Mette</td>\n",
              "      <td>Crossfit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Pernille</td>\n",
              "      <td>Crossfit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Pernille</td>\n",
              "      <td>Jomfru Anne Gade</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Jesper</td>\n",
              "      <td>Yoga House</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Dorte</td>\n",
              "      <td>Jazz Club</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Dorte</td>\n",
              "      <td>Yoga House</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       source            target\n",
              "0   Christian          Crossfit\n",
              "1        Lise  Jomfru Anne Gade\n",
              "2        Lise         Jazz Club\n",
              "3      Casper          Crossfit\n",
              "4      Casper        Yoga House\n",
              "5      Casper         Jazz Club\n",
              "6       Helle        Yoga House\n",
              "7       Helle          Crossfit\n",
              "8      Morten          Crossfit\n",
              "9       Mette          Crossfit\n",
              "10   Pernille          Crossfit\n",
              "11   Pernille  Jomfru Anne Gade\n",
              "12     Jesper        Yoga House\n",
              "13      Dorte         Jazz Club\n",
              "14      Dorte        Yoga House"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Crosstab and dot product"
      ],
      "metadata": {
        "id": "X7IUidYetpEM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zw6MR3sXqvZ"
      },
      "source": [
        "# create matrix from edges\n",
        "adj_df = pd.crosstab(edges_df.source, edges_df.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnNn0k7rxkhH",
        "outputId": "021051ef-7ed9-40c6-8376-6feff5f7e37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "adj_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>target</th>\n",
              "      <th>Crossfit</th>\n",
              "      <th>Jazz Club</th>\n",
              "      <th>Jomfru Anne Gade</th>\n",
              "      <th>Yoga House</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>source</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Casper</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Christian</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dorte</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Helle</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jesper</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lise</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mette</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Morten</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pernille</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "target     Crossfit  Jazz Club  Jomfru Anne Gade  Yoga House\n",
              "source                                                      \n",
              "Casper            1          1                 0           1\n",
              "Christian         1          0                 0           0\n",
              "Dorte             0          1                 0           1\n",
              "Helle             1          0                 0           1\n",
              "Jesper            0          0                 0           1\n",
              "Lise              0          1                 1           0\n",
              "Mette             1          0                 0           0\n",
              "Morten            1          0                 0           0\n",
              "Pernille          1          0                 1           0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GR-7hHXYV2B",
        "outputId": "ba46ee3a-80a5-49c2-861e-0682dc02f5d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# Projecting with dot-product\n",
        "pd.DataFrame(np.dot(adj_df, adj_df.T), \n",
        "             index=adj_df.index, \n",
        "             columns=adj_df.index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>source</th>\n",
              "      <th>Casper</th>\n",
              "      <th>Christian</th>\n",
              "      <th>Dorte</th>\n",
              "      <th>Helle</th>\n",
              "      <th>Jesper</th>\n",
              "      <th>Lise</th>\n",
              "      <th>Mette</th>\n",
              "      <th>Morten</th>\n",
              "      <th>Pernille</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>source</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Casper</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Christian</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dorte</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Helle</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jesper</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lise</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mette</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Morten</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pernille</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "source     Casper  Christian  Dorte  Helle  ...  Lise  Mette  Morten  Pernille\n",
              "source                                      ...                               \n",
              "Casper          3          1      2      2  ...     1      1       1         1\n",
              "Christian       1          1      0      1  ...     0      1       1         1\n",
              "Dorte           2          0      2      1  ...     1      0       0         0\n",
              "Helle           2          1      1      2  ...     0      1       1         1\n",
              "Jesper          1          0      1      1  ...     0      0       0         0\n",
              "Lise            1          0      1      0  ...     2      0       0         1\n",
              "Mette           1          1      0      1  ...     0      1       1         1\n",
              "Morten          1          1      0      1  ...     0      1       1         1\n",
              "Pernille        1          1      0      1  ...     1      1       1         2\n",
              "\n",
              "[9 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Networkx"
      ],
      "metadata": {
        "id": "a7MaqCB8IO66"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMsvPsKcYnDJ",
        "outputId": "7000fd02-1361-4408-9ab3-8ba6d2eb0f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "# with nx\n",
        "nx.to_pandas_adjacency(B_people)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Christian</th>\n",
              "      <th>Lise</th>\n",
              "      <th>Casper</th>\n",
              "      <th>Helle</th>\n",
              "      <th>Morten</th>\n",
              "      <th>Mette</th>\n",
              "      <th>Pernille</th>\n",
              "      <th>Jesper</th>\n",
              "      <th>Dorte</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Christian</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lise</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Casper</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Helle</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Morten</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mette</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pernille</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jesper</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dorte</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Christian  Lise  Casper  Helle  ...  Mette  Pernille  Jesper  Dorte\n",
              "Christian        0.0   0.0     1.0    1.0  ...    1.0       1.0     0.0    0.0\n",
              "Lise             0.0   0.0     1.0    0.0  ...    0.0       1.0     0.0    1.0\n",
              "Casper           1.0   1.0     0.0    2.0  ...    1.0       1.0     1.0    2.0\n",
              "Helle            1.0   0.0     2.0    0.0  ...    1.0       1.0     1.0    1.0\n",
              "Morten           1.0   0.0     1.0    1.0  ...    1.0       1.0     0.0    0.0\n",
              "Mette            1.0   0.0     1.0    1.0  ...    0.0       1.0     0.0    0.0\n",
              "Pernille         1.0   1.0     1.0    1.0  ...    1.0       0.0     0.0    0.0\n",
              "Jesper           0.0   0.0     1.0    1.0  ...    0.0       0.0     0.0    1.0\n",
              "Dorte            0.0   1.0     2.0    1.0  ...    0.0       0.0     1.0    0.0\n",
              "\n",
              "[9 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Join/Merge the dataset with itself"
      ],
      "metadata": {
        "id": "TOjQelLvILFu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "dzvz8V4nPiDo",
        "outputId": "adba6122-3ecb-4d67-c099-e6c4094e6f5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Authors_x  Authors_ID_x  \\\n",
              "0  Dandachi G.   56705485700   \n",
              "1  Dandachi G.   56705485700   \n",
              "2  Dandachi G.   56705485700   \n",
              "3  Dandachi G.   56705485700   \n",
              "4  Dandachi G.   56705485700   \n",
              "\n",
              "                                               Title Country_x  Topic_x  \\\n",
              "0  A robust control-theory-based exploration stra...    France      2.0   \n",
              "1  A robust control-theory-based exploration stra...    France      2.0   \n",
              "2  A robust control-theory-based exploration stra...    France      2.0   \n",
              "3  A robust control-theory-based exploration stra...    France      2.0   \n",
              "4  A robust control-theory-based exploration stra...    France      2.0   \n",
              "\n",
              "          Authors_y  Authors_ID_y Country_y  Topic_y  \n",
              "0       Dandachi G.   56705485700    France      2.0  \n",
              "1           Cerf S.   57192995977    France      2.0  \n",
              "2   Hadjadj-Aoul Y.   55663651600    France      2.0  \n",
              "3     Outtagarts A.   55416314300    France      2.0  \n",
              "4         Rutten E.   56276265500    France      2.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f514a9b0-cdfc-4829-be74-08c5bbf376bb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Authors_x</th>\n",
              "      <th>Authors_ID_x</th>\n",
              "      <th>Title</th>\n",
              "      <th>Country_x</th>\n",
              "      <th>Topic_x</th>\n",
              "      <th>Authors_y</th>\n",
              "      <th>Authors_ID_y</th>\n",
              "      <th>Country_y</th>\n",
              "      <th>Topic_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dandachi G.</td>\n",
              "      <td>56705485700</td>\n",
              "      <td>A robust control-theory-based exploration stra...</td>\n",
              "      <td>France</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Dandachi G.</td>\n",
              "      <td>56705485700</td>\n",
              "      <td>France</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dandachi G.</td>\n",
              "      <td>56705485700</td>\n",
              "      <td>A robust control-theory-based exploration stra...</td>\n",
              "      <td>France</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Cerf S.</td>\n",
              "      <td>57192995977</td>\n",
              "      <td>France</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dandachi G.</td>\n",
              "      <td>56705485700</td>\n",
              "      <td>A robust control-theory-based exploration stra...</td>\n",
              "      <td>France</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Hadjadj-Aoul Y.</td>\n",
              "      <td>55663651600</td>\n",
              "      <td>France</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dandachi G.</td>\n",
              "      <td>56705485700</td>\n",
              "      <td>A robust control-theory-based exploration stra...</td>\n",
              "      <td>France</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Outtagarts A.</td>\n",
              "      <td>55416314300</td>\n",
              "      <td>France</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dandachi G.</td>\n",
              "      <td>56705485700</td>\n",
              "      <td>A robust control-theory-based exploration stra...</td>\n",
              "      <td>France</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Rutten E.</td>\n",
              "      <td>56276265500</td>\n",
              "      <td>France</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f514a9b0-cdfc-4829-be74-08c5bbf376bb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f514a9b0-cdfc-4829-be74-08c5bbf376bb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f514a9b0-cdfc-4829-be74-08c5bbf376bb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "# Merging the data with itself using Title as key\n",
        "edges = pd.merge(data_select, data_select, on='Title')\n",
        "edges.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted network projection"
      ],
      "metadata": {
        "id": "Kr3b6TuguHpF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S92TTSjSr2jD"
      },
      "source": [
        "It is possible to discount for the number of nodes when projecting weighted two-mode networks.\n",
        " \n",
        " For example, it could be argued that if many online users post to a thread, their ties should be weaker than if there were few people posting to the thread. A straight forward generalisation is the following function: $w_{ij} = \\sum_p \\frac{w_{i,p}}{N_p - 1}$. \n",
        " \n",
        " This formula would create a directed one-mode network in which the out-strength of a node is equal to the sum of the weights attached to the ties in the two-mode network that originated from that node. For example, node C has a tie with a weight of 5 in the two-mode network and an out-strength of 5 in the one-mode projection.\n",
        "\n",
        "![](https://toreopsahl.files.wordpress.com/2009/04/fig1_twomode_forum_newman2001.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing"
      ],
      "metadata": {
        "id": "GClPkVQAtIP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elements of text vs. Text as such"
      ],
      "metadata": {
        "id": "THaFEZ3w74Sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interest of analysis goes into the direction of: Either were analysing the text as such (what is the statement in the text) or elements of text.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-11-06%20at%2013.31.59.png)\n"
      ],
      "metadata": {
        "id": "8AELOs5KtQqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing concepts"
      ],
      "metadata": {
        "id": "u4fpxowcJEMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntax and semantics. The syntax is the grammatical structure of the text, and semantics is the meaning being conveyed.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes//main/Images/vURoVPyiqdRaOmec.png)"
      ],
      "metadata": {
        "id": "0sE_GfUjRo1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "q0Al0q-4RiVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokenization separates a piece of text into smaller units called tokens\n",
        "- Tokenization can be broadly classified into 3 types:\n",
        "  - word tokens: smarter\n",
        "  - character tokens: s-m-a-r-t-e-r\n",
        "  - subword token (n-gram characters): smart-er\n",
        "- But different methods may be applied when tokenizing\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/tokenize.png)"
      ],
      "metadata": {
        "id": "Vf8UDeJtRtEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization"
      ],
      "metadata": {
        "id": "yaxqxgQQSgQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Reduces randomness of text, bringing it closer to a predefined 'standard, i.e. improving efficiency\n",
        "- Before normalizing:\n",
        "  - expand contractions (by making a dictionary)\n",
        "  - tokenize\n",
        "  - remove punctuations\n",
        "- Normalization techniques:\n",
        "  - Stemming: reducing words to their word stem or root form (may not be a dictionary word)\n",
        "    - Over-stemming: more than required is removed, e.g. “university” and “universe” = “univers”.\n",
        "    - Under-stemming: less than required is removed, e.g. “data” and “datum” = “dat” and “datu” (instead of “dat”).\n",
        "  - Lemmatization: reducing words to their base word in the language (is in the dictionary)\n",
        "    - A root word is called lemma. A lemma is the canonical form, dictionary form, or citation form of a set of words.\n",
        "\n",
        "\n",
        "Should you always normalize?\n",
        "\n",
        "No, it depends on the problem.\n",
        "- Lemmatization is needed for topic modeling and for training word vectors, which is dependent of string matches and accurate word counts\n",
        "- Semantic analysis methods on the other hand have different ratings depending on the form of the word and therefore the input should not be stemmed or lemmatized.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/pLfTViDgRXuHYvWE-2.png)"
      ],
      "metadata": {
        "id": "Lr_YIpbGSh0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequency based dictionary filtering"
      ],
      "metadata": {
        "id": "zk0uFyYNSj3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Removing stop words, i.e. filtering out high-frequency words that add little or no semantic value to a sentence, e.g.:\n",
        "  - to, for, on, and, the, etc.\n",
        "  - You can even create custom lists of stopwords to include words that you want to ignore."
      ],
      "metadata": {
        "id": "y4tH7-ljSmLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS tagging (Part-of-Speech) (TAGGER)"
      ],
      "metadata": {
        "id": "MDYZRLBQSpu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Assigning each word (token) in a sentence the part of speech category that it assumes in that sentence.\n",
        "- Target is to identify the grammatical group of a given word: noun, pronoun, adjective, verb, adverbs, etc. based on the context.\n",
        "- POS tagging improves accuracy, .e.g ‘leaves’ without a POS tag would get lemmatized to ‘leaf’, but with a verb tag, its lemma would be ‘leave’.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/GVCinaTpbVEEOukr.png)"
      ],
      "metadata": {
        "id": "qWkuy6g2Sshl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependency Parsing (PARSER)"
      ],
      "metadata": {
        "id": "wHhn4eeESvBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency grammar refers to the way the words in a sentence are connected. A dependency parser, therefore, analyzes how ‘head words’ are related and modified by other words to understand the syntactic structure of a sentence.\n",
        "\n",
        "nsubj: nominal subject\n",
        "\n",
        "dobj: direct object\n",
        "\n",
        "conj: conjugation\n",
        "\n",
        "advmod: adverbial modifier\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/aEFHscfTPxihutra.png)"
      ],
      "metadata": {
        "id": "pwPLD5l9Sw2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "q7sIdF09S0-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Extracts entities from text documents, e.g. names, places, organizations, email addresses, etc.\n",
        "- Relationship extraction takes this one step further and finds relationships between two nouns. e.g. “Lumiere lives in Nice,” a person (Lumiere) is related to a place (Nice) by the semantic category “lives in.”\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/AVySmceRpFKFjcaM.png)\n",
        "\n",
        "RJ example: From text to network\n",
        "\n",
        "There may be relationships between entitites in the text which can be translated to represent relationships with network structures. It may even be directed relationships\n",
        "\n",
        "How to get there:\n",
        "\n",
        "Look at grammar. What are the subjects, objects, and how are they related. Packages exist for this, turning texts into elements and performing analysis on the elements. I.e. representing text as a relational structure, creating an edgelist, nodelist, etc.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-11-06%20at%2014.02.49.png)"
      ],
      "metadata": {
        "id": "0WgZCHEYLPvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization approaches"
      ],
      "metadata": {
        "id": "MNMrgP_HJNUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of words (BoW) (one-hot enc)"
      ],
      "metadata": {
        "id": "vDodESNHarPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to apply ML approaches, we need tabular data where:\n",
        "- rows = text = each sentence\n",
        "- columns = features = f1 to fn is the vocabulary contained in the whole collection of texts, i.e. all words used in the sentences\n",
        "\n",
        "Example:\n",
        "\n",
        "the fat cat sits on the mat\n",
        "\n",
        "the cat ate a fat rat\n",
        "\n",
        "the dog is sad\n",
        "\n",
        "Procedure:\n",
        "- Kick out words that don't carry much meaning\n",
        "  \n",
        "  - e.g. articles the, is, a etc. preprositions and full stops.\n",
        "\n",
        "- Normalize text: stemming or lemmatizing\n",
        "  - Normalizing the vocab by removing gender, declension and conjugation. Changes the verbs according to whether is plural, singular, etc. and transform past tense into present.\n",
        "  - Reason for normalizing: Discovering meaning is the same in normalized text, as long as what you want to figure out is related to meaning and not so much to gender or time etc. (in that case do not normliaze)\n",
        "\n",
        "- Turn the vocab into a table\n",
        "  - Rows: Text (sentences)\n",
        "  - Columns: Features (words)\n",
        "\n",
        "- Add 1's and 0's\n",
        "  - depending if the word is present in the sentence or not. Result is a sparse matrix representing text in tabular form\n",
        "\n",
        "- Apply ML models\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2016.51.35.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.56.59.png)"
      ],
      "metadata": {
        "id": "Sg7UEn2warPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "nVEdUk23bNTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word embeddings represent words in the form of vectors that encodes the meaning of the word based om semantic and syntactic features, meaning that words that are closer in the vector space are expected to be similar in meaning.\n",
        "\n",
        "Rational/mechanism behind word embeddings/vectors:\n",
        "- The distributional hypopothesis \"You shall know a word by the company it keeps\", i.e. the meaning of a word is defined by the context in which it tends to occur. By observing the words surrounding a word will give us an understanding of what the word means.\n",
        "\n",
        "Embedding methods are: \n",
        "- Word2vec\n",
        "- Fasttext\n",
        "- TF-IDF\n",
        "\n",
        "Word embeddings can also be usen on non-sequential inputs, e.g. recipes used to create food-vectors.\n",
        "\n",
        "Example of word embeddings in practice.\n",
        "\n",
        "In simple terms, researchers estimated word vectors from textual inputs in different time-frames. Pikcing terms and person that changed *their company* over the years. Then they look at the relative position of terms compared to terms that did not change much (anchors)\n",
        "\n",
        "![alt text](https://adriancolyer.files.wordpress.com/2018/02/evolving-word-embeddings-fig-1.jpeg)\n"
      ],
      "metadata": {
        "id": "EIVgXb7b6TYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word2Vec"
      ],
      "metadata": {
        "id": "nEz4vpwxOYS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The entire corpus is scanned\n",
        "- The vector is created by determining which words the target word occurs with more often\n",
        "- I.e. the semantic closeness between the words are expressed mathematically in the vector\n",
        "\n",
        "Word2vec algorithm formalizes:\n",
        "You look at the first three words and try to predict the 4th word. Once you're done with that you move the window by one and try to predict word 5 etc.\n",
        "\n",
        "The model is a shallow naural network:\n",
        "- Running the prediction iteratively identified patterns that map the relationship between terms and all possible contexts, resulting in a vector representation (word embedding) of words carrying that meaning. Uses:\n",
        "  - Similar terms have similar vectors\n",
        "  - Vberlin - Vparis + Vbeer = Vwine, it captures not only the words but also the context and allows you to do linear algebra on the words and find underlying latent structures\n",
        "  - Average vector of all the words in a sentence as a document representation\n",
        "  - Input for deep learning applications\n",
        "    - What if were not only looking at words in a sentence but feeding the model with vectors that represent the words in the right order. Feeding word vectors into a neural network into specific models, going deeper into meaning structures.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2008.36.37.png)\n",
        "\n",
        "The Word2vec model captures both syntactic and semantic similarities between the words. One of the well known examples of the vector algebraic on the trained word2vec vectors is:\n",
        "\n",
        "Vector(“King”)-Vector(“Man”)= Vector(“Queen”)-Vector(“Woman)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/1*5F4TXdFYwqi-BWTToQPIfg.jpeg)"
      ],
      "metadata": {
        "id": "ZFDnflk6xkUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FastText"
      ],
      "metadata": {
        "id": "J9vyo3N5P0ng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fasttext extends the idea of Word2Vec, enriching the vectors by information from sub-word elements.\n",
        "\n",
        "What does that mean?\n",
        "\n",
        "Words are not only defined by surrounding words but in addition also by the various syllables that make up the word. Why should that be a good idea? Well, now words such as *apple* and *apples* do not only get similar vectors due to them often sharing context but also because they are composed of the same sub-word elements. This comes in particularly handy when we are dealing with language that have a rich morphology such as Turkish or Russian.  This is also great when working with web-text, which is often messy and misspelt.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/fastText-vs.-Word2Vec.png)"
      ],
      "metadata": {
        "id": "WUBLVQ6Y-P9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer models"
      ],
      "metadata": {
        "id": "z6fvIeDx_HSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The current state-of-the-art transformer models go even further and implement context-specificity (a word may change meaning depending on the context in which it occurs)\n",
        "\n",
        "Transformer models apply an evolving set of mathematical techniques, called attention or self-attention, to detect subtle ways even distant data elements in a series influence and depend on each other."
      ],
      "metadata": {
        "id": "SZwnmjJN_JnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality reduction and topic modeling"
      ],
      "metadata": {
        "id": "qYiVPvZdJsFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSI / LSA / SVD / NMF / UMAP / (PCA?)"
      ],
      "metadata": {
        "id": "d0XlQqF3arPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW and TF-IDF is sufficient for many SML approaches, e.g. classification or sentiment analysis. Performance can be alright with BoW but better with TF-IDF.\n",
        "\n",
        "Going from sparse matrix to dense:\n",
        "- LSI (latent semantic indexing) or LSA (latent semantic analysis)\n",
        "- Similar to SVD (singular value decomposition) or NMF (non negative matrix) in terms of mathematics and output\n",
        "- UMAP is used to reduce word vectors to 2 dimensions for visualizations\n",
        "- Should PCA really be used in NLP? Not mentioned in any notebooks\n",
        "\n",
        "Dimensionality reduction example (LSI/LSA):\n",
        "- We have a sparse matrix using BoW or onehot-encoding (i.e. docs as rows and words a columns)\n",
        "- Transforming the matrix to dense using LSI/LSA\n",
        "- The result is one matrix with documents vs. topics and one matrix with topics vs. vocab\n",
        "  - documents vs. topics: dense matrix that identifies topics, similar to UML, i.e. components. Topics are easy to interpret because the way we speak and write is logical. Linear algebra can help uncover latent topics in text corpora. Each document is represented as a combination of topics.\n",
        "  - topics vs. vocab: dense matrix that look at to which extent terms contribute to topics. What is the relationship between certain terms and certain topics. This is a lower dimension matrix that can be used for:\n",
        "      - SML\n",
        "      - Similarity (e.g. cosine similarity. The advantage of similarity based on reduced matrices compared to BoW/TF-IDF is that you are able to get similarity of two documents that don't share any words but contains certain synonyms. With BoW you need to have exact common terms, LSI/LSA on the other hand is based on topics and not words, i.e. getting closer to semantic similarity.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2008.14.36.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "BnNaGt5FarPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF (Term Frequency - Inverse Document Frequency)"
      ],
      "metadata": {
        "id": "2qyE_ssearPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforms text to sparse matrix\n",
        "\n",
        "TF-IDF tries to weigh words, i.e. if a word appears in all texts it may not be as important for the individual texts. Original words are probably more important than general words. e.g. the word 'president' in political tweets.\n",
        "\n",
        "The weight of term x and sentence y is equal to the term frequency of the word x in sentence y times the logarithm of the total number of documents divided by dfx (the number of documents the word x appears in, i.e. cannot be larger than N).\n",
        "\n",
        "The closer log is to one, the lower the importance of the word\n",
        "\n",
        "i.e. TF-IDF discount general terms / highlight specific terms. In reality just looking a frequency distribution.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2008.01.35.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "ymjA6MrzarPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA"
      ],
      "metadata": {
        "id": "4Lcg_EplD2KV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA is a probabilistic model (LSI/LSA is algebraic, faster to train but lower accuracy)\n",
        "\n",
        "LDA shouldn't be used as SML input, but is great for topic discorey and visualizations\n",
        "\n",
        "Example of LDA:\n",
        "\n",
        "Suppose you have the following set of sentences:\n",
        "- I ate a banana and spinach smoothie for breakfast\n",
        "- I like to eat broccoli and bananas.\n",
        "- Chinchillas and kittens are cute.\n",
        "- My sister adopted a kitten yesterday.\n",
        "- Look at this cute hamster munching on a piece of broccoli.\n",
        "\n",
        "Latent Dirichlet allocation is a way of automatically discovering topics that these sentences contain. For example, given these sentences and asked for 2 topics, LDA might produce something like:\n",
        "- Sentences 1 and 2: 100% Topic A\n",
        "- Sentences 3 and 4: 100% Topic B\n",
        "- Sentence 5: 60% Topic A, 40% Topic B\n",
        "- Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, ... (at which point, you could interpret topic A to be about food)\n",
        "- Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, ... (at which point, you could interpret topic B to be about animals)\n",
        "\n",
        "\n",
        "Coherence Score:\n",
        "- We can use the coherence score in topic modeling to measure how interpretable the topics are to humans. In this case, topics are represented as the top N words with the highest probability of belonging to that particular topic. Briefly, the coherence score measures how similar these words are to each other\n",
        "\n",
        "Word2vec topic modeling:\n",
        "- Topic modeling can also be performed based on Word2vec using the vectors as input in a clustering algorithm, resulting in a group of clusters, each representing a topic.This produces similar but less accurate results than LDA\n",
        "\n",
        "Corex:\n",
        "- Can we make existing approaches more informative\n",
        "- Anchor words - means that you get some kind of semi-supervised  representation. So you can infuse the model with some domain expertise. E.g. I know there are some topics in this corpus and I know these are terms that belong to the different topics, so take these into account when you are making the groupings automatically"
      ],
      "metadata": {
        "id": "1hBbzbyGG7xW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP in supervised pipelines"
      ],
      "metadata": {
        "id": "v3dSCMhgJzsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiating models and \"bundle it up as a SML pipeline\""
      ],
      "metadata": {
        "id": "Zx4B-7QqFu6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate models and \"bundle up as pipeline\" using sklearn make_pipeline\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "cls = LogisticRegression()\n",
        "\n",
        "pipe = make_pipeline(tfidf, cls)"
      ],
      "metadata": {
        "id": "hZl3ys9WFKTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(X_train,y_train) # fit model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okQT5dM3E1C1",
        "outputId": "d8095990-ed26-4601-97f2-fab25da57e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
              "                ('logisticregression', LogisticRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model performance on training set\n",
        "\n",
        "y_eval = pipe.predict(X_train)\n",
        "report = classification_report(y_train, y_eval)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ne8wlHfHfyL",
        "outputId": "ef5fed38-6df7-4dde-aac5-3801e9c185ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.70      0.78     11291\n",
            "           1       0.84      0.94      0.89     18709\n",
            "\n",
            "    accuracy                           0.85     30000\n",
            "   macro avg       0.86      0.82      0.83     30000\n",
            "weighted avg       0.85      0.85      0.85     30000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explainability (Eli5)"
      ],
      "metadata": {
        "id": "cLXhwzdTJ1_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Eli5 it is possible to visualize which words (features) contribute to which topics. I.e. if a text includes specific words the model will place it in that category/classification"
      ],
      "metadata": {
        "id": "imb-ElR0E_qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overall weights (works only for linear models)\n",
        "eli5.show_weights(pipe, top=10, target_names=['hate','offensive','nothing'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "WN-oI_JwinYC",
        "outputId": "c07265bb-a91f-4b57-abf9-68089233b2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights-wrapper\" style=\"border-collapse: collapse; border: none; margin-bottom: 1.5em;\">\n",
              "            <tr>\n",
              "                \n",
              "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
              "                        <b>\n",
              "    \n",
              "        y=hate\n",
              "    \n",
              "</b>\n",
              "\n",
              "top features\n",
              "                    </td>\n",
              "                \n",
              "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
              "                        <b>\n",
              "    \n",
              "        y=offensive\n",
              "    \n",
              "</b>\n",
              "\n",
              "top features\n",
              "                    </td>\n",
              "                \n",
              "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
              "                        <b>\n",
              "    \n",
              "        y=nothing\n",
              "    \n",
              "</b>\n",
              "\n",
              "top features\n",
              "                    </td>\n",
              "                \n",
              "            </tr>\n",
              "            <tr>\n",
              "                \n",
              "                    \n",
              "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
              "                            \n",
              "                                \n",
              "                                    \n",
              "                                    \n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
              "                    Weight<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.53%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.612\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        white\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.435\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        faggot\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.36%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.297\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        nigger\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.16%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.006\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        nigga\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 89.26%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.613\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        fag\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 89.61%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.494\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        faggots\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 89.91%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.392\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        fags\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 90.10%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.328\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        niggas\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 90.40%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.227\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        ass\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 90.41%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.225\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        niggers\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 90.41%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 1806 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 3830 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "                                \n",
              "                            \n",
              "                        </td>\n",
              "                    \n",
              "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
              "                            \n",
              "                                \n",
              "                                    \n",
              "                                    \n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
              "                    Weight<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +6.354\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        bitch\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.08%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.785\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        pussy\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.14%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.763\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        bitches\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.40%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.662\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        hoes\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.61%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.842\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        hoe\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.76%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.791\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        shit\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 92.40%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.596\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        ai\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 92.40%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 1789 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 93.55%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 3847 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 93.55%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.262\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        charlie\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 93.15%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.374\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        white\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 93.11%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.386\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        trash\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "                                \n",
              "                            \n",
              "                        </td>\n",
              "                    \n",
              "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
              "                            \n",
              "                                \n",
              "                                    \n",
              "                                    \n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
              "                    Weight<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 89.03%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.695\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        charlie\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 89.54%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.517\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        bird\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 89.54%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 2790 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 90.37%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 2846 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 90.37%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.238\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        white\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 90.34%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.246\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        nigger\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 89.16%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.648\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        hoe\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 88.71%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.808\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        faggot\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 88.48%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.890\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        pussy\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 87.51%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -3.242\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        hoes\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 86.93%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -3.460\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        bitches\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.84%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -5.977\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        bitch\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "                                \n",
              "                            \n",
              "                        </td>\n",
              "                    \n",
              "                \n",
              "            </tr>\n",
              "        </table>\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# explain one prediction\n",
        "eli5.show_prediction(pipe[1], t1_p[0], vec=pipe[0],\n",
        "                     target_names=['hate','offensive','nothing'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "2TJSO7JLjwZp",
        "outputId": "a83985dd-28b7-4a26-e471-86c6968caa94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=hate\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.674</b>, score <b>2.005</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.55%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.180\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        Highlighted in text (sum)\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 97.70%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.175\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"background-color: hsl(120, 100.00%, 83.13%); opacity: 0.86\" title=\"0.629\">stupid</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 66.47%); opacity: 0.96\" title=\"1.679\">fag</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.46%); opacity: 0.81\" title=\"-0.128\">bitch</span>\n",
              "    </p>\n",
              "\n",
              "    \n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=offensive\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.323</b>, score <b>1.269</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.88%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.661\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        Highlighted in text (sum)\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 95.95%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.392\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"background-color: hsl(0, 100.00%, 98.89%); opacity: 0.80\" title=\"-0.013\">stupid</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.93%); opacity: 0.84\" title=\"-0.486\">fag</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"2.160\">bitch</span>\n",
              "    </p>\n",
              "\n",
              "    \n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=nothing\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.003</b>, score <b>-3.274</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 94.76%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.567\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -3.841\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        Highlighted in text (sum)\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"background-color: hsl(0, 100.00%, 83.37%); opacity: 0.86\" title=\"-0.616\">stupid</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 73.60%); opacity: 0.91\" title=\"-1.193\">fag</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 61.68%); opacity: 0.99\" title=\"-2.032\">bitch</span>\n",
              "    </p>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['tweet'][100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IxKdidW1mW25",
        "outputId": "019902cb-6717-4776-b079-0acdecd3c7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"@ClicquotSuave: LMAOOOOOOOOOOO this nigga @Krillz_Nuh_Care http://t.co/AAnpSUjmYI\" &lt;bitch want likes for some depressing shit..foh'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['class'][100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs3wH9DTmiJx",
        "outputId": "b621f55e-bcd6-4af6-ea0b-69e7819a6bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eli5.show_prediction(pipe[1], data['text_clean'][100], vec=pipe[0],\n",
        "                     target_names=['hate','offensive','nothing'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "kw5JSFnglfS9",
        "outputId": "6773e1ce-c411-43b8-c82d-adf281e127b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=hate\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.690</b>, score <b>1.079</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 83.29%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.254\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        Highlighted in text (sum)\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 95.79%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.175\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"background-color: hsl(120, 100.00%, 82.72%); opacity: 0.86\" title=\"0.238\">lmaooooooooooo</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.790\">nigga</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.72%); opacity: 0.82\" title=\"-0.083\">want</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.94%); opacity: 0.86\" title=\"0.234\">likes</span><span style=\"opacity: 0.80\"> depressing </span><span style=\"background-color: hsl(120, 100.00%, 89.19%); opacity: 0.83\" title=\"0.122\">shit</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.45%); opacity: 0.81\" title=\"-0.047\">foh</span>\n",
              "    </p>\n",
              "\n",
              "    \n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=offensive\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.229</b>, score <b>-0.026</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 92.93%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.367\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        Highlighted in text (sum)\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 92.59%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.392\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"background-color: hsl(0, 100.00%, 95.16%); opacity: 0.81\" title=\"-0.039\">lmaooooooooooo</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 81.78%); opacity: 0.87\" title=\"-0.257\">nigga</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.80%); opacity: 0.82\" title=\"0.068\">want</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.00%); opacity: 0.80\" title=\"0.011\">likes</span><span style=\"opacity: 0.80\"> depressing </span><span style=\"background-color: hsl(120, 100.00%, 70.94%); opacity: 0.93\" title=\"0.501\">shit</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.78%); opacity: 0.82\" title=\"0.082\">foh</span>\n",
              "    </p>\n",
              "\n",
              "    \n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=nothing\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.082</b>, score <b>-1.053</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 90.41%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.567\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.620\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        Highlighted in text (sum)\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"background-color: hsl(0, 100.00%, 84.73%); opacity: 0.85\" title=\"-0.200\">lmaooooooooooo</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 69.63%); opacity: 0.93\" title=\"-0.533\">nigga</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.49%); opacity: 0.80\" title=\"0.015\">want</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.39%); opacity: 0.86\" title=\"-0.245\">likes</span><span style=\"opacity: 0.80\"> depressing </span><span style=\"background-color: hsl(0, 100.00%, 66.15%); opacity: 0.96\" title=\"-0.622\">shit</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.44%); opacity: 0.81\" title=\"-0.036\">foh</span>\n",
              "    </p>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Typical applications"
      ],
      "metadata": {
        "id": "8r0f2fWMaB8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Industry Applications Of NLP\n",
        "- News Aggregation\n",
        "- Social Media Monitoring\n",
        "- Analytics For Marketing\n",
        "- Automated Customer Service\n",
        "- Content Filtering- Spam filters\n",
        "- Semantic search\n",
        "- Text Classification\n",
        "  - Semantic analysis\n",
        "  - Intent detection\n",
        "  - Topic modeling\n",
        "  - Language detection.\n",
        "\n",
        "\n",
        "Chatbots\n",
        "- Train a model on variants of a question.\n",
        "- Take input and predict the type of question asked - this is called “intent”\n",
        "  - Consider similarity-based approaches, where input text is matched with most \"similar\" predifined questions and then the user picks upon request - \"did you mean xyz-question\"\n",
        "  - Alternatively, one could create paraphrased versions of questions for each question-answer pair to have more training examples\n",
        "- Reply with a pre-defined response corresponding to the question asked.\n",
        "- Chat-bot using:\n",
        "  - Spacy\n",
        "  - Floret (based on FastText and Bloom)"
      ],
      "metadata": {
        "id": "pME_8M70J45V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comments on our project"
      ],
      "metadata": {
        "id": "KeMigNJ0gxhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic modeling"
      ],
      "metadata": {
        "id": "Ex2VREcbg0vi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Preprocessing:\n",
        "  - Using spacy to preprocess text\n",
        "\n",
        "2. Tokenizing:\n",
        "  - lemmaticing and lowering\n",
        "  keeping only noun, propn, adj, adv\n",
        "\n",
        "3. creating a dictionary from the tokens. filtering out words that appear less than five times and removing words that appear in more than 50% of documents, i..e no need for TF-IDF, keeping only 1000 words\n",
        "\n",
        "4. construction the corpus from the dictionary using BoW, i.e. converting document (a list of words) into bag-of-words-format, i.e. token_id and token_count\n",
        "\n",
        "5. applying it to the LDA model using coprus, dictionary, setting number of topics to 5\n",
        "\n",
        "6. printing topics and their keywords, showing how much each word contribute to each topic\n",
        "\n",
        "7. Visualising using PyLDAviz\n",
        "\n",
        "8. Qualitatively interpreting the topics based on the keywords and our knowledge.\n",
        "\n",
        "9. Topics over time. Creating a function to limit the corpus to each year, using the same model and the same dictionary.\n",
        "\n",
        "10. Optimal topics. Basically creates a model for 1-25 topics and displays it in a graph, calculating Jaccard similarity of topics, the goal is a low score for coverage the diverse elements. calculating coherence\n",
        "\n",
        "Discussion points:\n",
        "Topic modeling:\n",
        "  - Why 5 topics when the graph says 14?\n",
        "  For domain exports the combination of words may indicate different topics, however, for us as researchers without domain expertise, 14 topics makes the distinction difficult.\n",
        "  - combining words, e.g. artificial intelligence, machine learning etc. they're split in the model. bi-grams\n",
        "  - Topics over the years:\n",
        "    - Should a new model be trained for each year, using year_LDA, year_corpus, year_dictionary?\n",
        "    - Should the same model be used and year_corpus and year_dictionary?\n",
        "    - pros and cons: yes, arguably you should create and new model and ditionary for each year, since the vocab and terms may develop over time, i.e. the model in 2012 includes terms that may not exist at this point, however, the advantage is that it makes comparison easier since it's based on the same foundation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_80lyxYnhKJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network analysis"
      ],
      "metadata": {
        "id": "SyR6C5hUg3XN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. dropping labels tha cause issues\n",
        "\n",
        "2. extractong countries. maybe could have used a package to impute countries??\n",
        "\n",
        "3. creating a dataframe identifyionh dominant topic. in tm more topics may be present depending on worss, but one is dominant.\n",
        "\n",
        "4. exploding and merging datasets\n",
        "\n",
        "5. constructing and edgelist with title as key\n",
        "\n",
        "6. creating weights by grouping author ids, creating counts\n",
        "\n",
        "7. creating the graph object as adding node attributes and calculations\n",
        "\n",
        "8. network analysis, turning the graph object back into a dataframe.\n",
        "\n",
        "9. value counts, communities using louvain, topics, countries, no affiliations\n",
        "\n",
        "10. additional measures: degree, neighborhood (shortest path), assortiativity, density, transitivity. not diameter, mean path lenght and reciprocity.\n",
        "\n",
        "\n",
        "Discussions:\n",
        "- Why not impute author ids in rows that cause issues. because it would be very difficult to do and perhaps wrong, so it's safer to delete\n",
        "- exploding, could have included less columns from the beginning.\n",
        "why using ayhors id and not authors, same name\n",
        "- exploding vs. roman ai method\n",
        "- quick an dirty method vs. correct method, why title as key, could have used topic or country\n",
        "- why louvain and not other algorithms, because louvain performs well, has high accuracy and can handle weighted graphs\n",
        "- why not look into affiliations, because some of them included several and we don't know which ones are associated with thich author...\n",
        "- For diameter and mean path length maybe use subsets.\n",
        "- challenged with network visualization in nxviz, used pehi instead.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ic-pAtFqlWds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "34YfTSp7g43Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sample 2000 rows\n",
        "take authros and keywords to a list\n",
        "clean authors keywords\n",
        "clean abstracts\n",
        "n-grams (bi-gram, 1,2 words, e.g. artifialc intelligence)\n",
        "TF-IDF\n",
        "Using KMeans to cluster. 5th group was very small so not pissible to oversample to cluster 5 was dropped to just 4\n",
        "saving the clusters as labels\n",
        "visualizing using pca\n",
        "taking out the vectors and saving it at x'es\n",
        "putting it back into the authors, keyword list\n",
        "find the top keywords. after tfidf is a sparse matrix, group by the clusters, take the mean and the terms (feauter names part of tfidf)\n",
        "map the clusters, i.e. giving them names\n",
        "add a colum labels to x_y dataframe\n",
        "extract y = the labels\n",
        "check for imbalance in the clusters\n",
        "visualizing TF-IDF and KMeans\n",
        "supervised ML \n",
        "apply random undersampler, forced to undersample, can't really oversample text\n",
        "split in train, test\n",
        "make a pipeline with tfidf and cls, making cls balanced\n",
        "training the cls model\n",
        "checking performance on the training data\n",
        "checking performance on the test data\n",
        "trying to predict on the whole dataset if it can apply labels\n",
        "\n",
        "using uml first kmeans to cluster\n",
        "using sml logistic regression to classify\n",
        "\n",
        "topics for discussion:\n",
        "KMeans instead of LDA for creating labels and instead of nomad dataset\n",
        "why CLS and not other classification"
      ],
      "metadata": {
        "id": "FZo_HGCz9Ydn"
      }
    }
  ]
}