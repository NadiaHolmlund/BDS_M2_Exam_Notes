{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOwjnRVEYBPerF4iYEgoXkC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadiaHolmlund/BDS_M2_Exam_Notes/blob/main/BDS_M2_Exam_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports for examples"
      ],
      "metadata": {
        "id": "D3xWxeu0epkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "UnR0mrPwexUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets for examples"
      ],
      "metadata": {
        "id": "HETQzrBZerex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Analysis"
      ],
      "metadata": {
        "id": "Ydr8ZL3hAI-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a network?"
      ],
      "metadata": {
        "id": "tocpWFQsBCQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A network is a system of elements (nodes/vertices) and connections (edges/links) between them. Networks are used to present relational data and can be applied to many types of relationships between different types of elements.\n",
        "\n",
        "\n",
        "nodes: system theory jargon\n",
        "\n",
        "vertices: graph theory jargon\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Unknown)"
      ],
      "metadata": {
        "id": "1XTz7eXPBFAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of networks"
      ],
      "metadata": {
        "id": "41fG2OLjCef9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The content, meaning and interpretation of networks depends of the elements and relationships displayed. Types of networks includes:\n",
        "\n",
        "Social networks:\n",
        "- Nodes/vertices represent actors (persons, firms, other socially constructed entitites)\n",
        "\n",
        "- Edges/links represent relationships between actors (friendship, interaction, co-affiliation, similarity, etc)\n",
        "\n",
        "Other networks:\n",
        "- Chemistry: Interaction between molecules\n",
        "- Computer Science: The world-wide-web, inter- and intranet topologies\n",
        "- Biology: Food-web, ant-hives"
      ],
      "metadata": {
        "id": "b0kyFInMDGTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The possibilities to depict relational data are manifold, e.g.:\n",
        "\n",
        "Relations among persons\n",
        "- Kinship: mother of, wife of…\n",
        "- Other role based: boss of, supervisor of…\n",
        "- Affective: likes, trusts…\n",
        "- Interaction: give advice, talks to, retweets…\n",
        "- Affiliation: belong to same clubs, shares same interests…\n",
        "\n",
        "Relations among organizations\n",
        "- As corporate entities, joint ventures, strategic alliances\n",
        "- Buy from / sell to, leases to, outsources to\n",
        "- Owns shares of, subsidiary of\n",
        "- Via their members (Personnel flows, friendship…)"
      ],
      "metadata": {
        "id": "jppmXsJMEz0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relational data structures"
      ],
      "metadata": {
        "id": "bbZ0UBojFFGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Edgelist"
      ],
      "metadata": {
        "id": "AJZzWF2uKIz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A common form of storing relational data\n",
        "- An edgelist is a dataframe containing minimum two columns:\n",
        "  - column 1: Source node of a connection\n",
        "  - column 2: Target node of a connection\n",
        "- Nodes are typically identified by unique IDs\n",
        "- An edge list can also contain additional columns that describe **attributes** of the edges such as magnitude aspects for an edge. If the edges have a magnitude attribute the graph is considered **weighted** (e.g., number of interactions, strenght of friendship). \n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/maxresdefault.jpg)\n"
      ],
      "metadata": {
        "id": "SfndnuXOFG-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjacency matrix / Socio matrix"
      ],
      "metadata": {
        "id": "UnIn_-psKMXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Represented as a n*n matrix, where n stands for the number of elements (nodes/vertices) of which relationships should be represented\n",
        "- The value in the cell that intercepts row n and column m indicates if an edge is present (=1) or absent (=0).\n",
        "- An adjacency matrix can be produced by crosstabulating an edgelist\n",
        "\n",
        "Note the impact of directed vs. undirected vs. weighted networks on adjacency matrices\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Different-types-of-graphs-and-their-corresponding-adjacency-matrix-representations-The.ppm.png)"
      ],
      "metadata": {
        "id": "KbVA1BdfHr_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nodelist"
      ],
      "metadata": {
        "id": "jDX0Vbr3KQak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Contains information about the nodes, aka attributes (edgelist and adjacency matrix stores only connectivity patterns ***between*** nodes)\n",
        "  - e.g. name, gender, age, group etc.\n",
        "\n",
        "  ![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.00.45.png)"
      ],
      "metadata": {
        "id": "3VF4g8ioKR44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network graphs"
      ],
      "metadata": {
        "id": "R_5wYXeHMT09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph objects"
      ],
      "metadata": {
        "id": "WXvYfAgcMVRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tabular data dependency\n",
        "- Between observation dependency: Summary statistics of variables are between observations (column-wise) interdependent, meaning changing a value of some observation will change the corresponding variables summary statistics.\n",
        "- Within observation dependency: Summary statitics of variables are within observations (row-wise) interdependent, meaning changing a variable value might change summary statistics of the observation\n",
        "- Otherwise, values are (at least mathematically) independent"
      ],
      "metadata": {
        "id": "3v0C_DWiOwXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph data dependency\n",
        "- Above holds true, but graph data holds additional dependencies due to the relational structure of data.\n",
        "- E.g. adding/removing node(s) may imply adding/removing edge(s) and adding/removing edge(s) may change the characteristics of node(s), due to their relational interdependence\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.24.25.png)\n"
      ],
      "metadata": {
        "id": "HfKysRcZQChy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph concepts and terminology"
      ],
      "metadata": {
        "id": "Wj20Vib3MXeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The vertices ***u*** and ***v*** are called the end vertices of the edge ***(u,v)***\n",
        "- If two edges have the same end vertices they are ***Parallel***\n",
        "- An edge of the form ***(v,v)*** is a ***loop***\n",
        "- A Graph is ***simple*** if it has no parallel edges and loops\n",
        "- A Graph is said to be ***Empty*** if it has no edges. Meaning ***E*** is empty\n",
        "- A Graph is a ***Null Graph*** if it has no vertices. Meaning ***V*** and ***E*** is empty\n",
        "- Edges are ***Adjacent*** if they have a common vertex. Vertices are ***Adjacent*** if they have a common edge\n",
        "- The ***degree*** of the vertex ***v***, written as ***d(v)***, is the number of edges with v as an end vertex. By convention, we count a loop twice and parallel edges contribute separately\n",
        "- ***Isolated*** Vertices are vertices with degree 1.\n",
        "- A Graph is ***Complete*** if its edge set contains every possible edge between ALL of the vertices\n",
        "- A ***Walk*** in a Graph ***G = (V,E)*** is a finite, alternating sequence of the form ViEiViEi consisting of vertices and edges of the graph ***G***\n",
        "- A ***Walk*** is ***Open*** if the initial and final vertices are different. A ***Walk*** is ***Closed*** if the initial and final vertices are the same"
      ],
      "metadata": {
        "id": "1mRS7q9OR1y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of graphs"
      ],
      "metadata": {
        "id": "eycSQC-mhw8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Weigthed vs. Unweighted\n",
        "2. Directed vs. Undirected\n",
        "3. Unimodal vs. Multimodal\n",
        "4. Unidimensional vs. Multidimensional\n",
        "\n",
        "`networkx` graph classes\n",
        "1. Graph\n",
        "2. DiGraph\n",
        "3. MultiGraph\n",
        "4. MultiDigraph"
      ],
      "metadata": {
        "id": "G9ShadZ6h009"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Weigthed vs. Unweighted"
      ],
      "metadata": {
        "id": "9np0YgUElKDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted: edges have values associated with them\n",
        "\n",
        "Unweighted: edges either exist or do not\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.48.07.png)"
      ],
      "metadata": {
        "id": "qKpZ7ORTgdEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Directed vs. Undirected"
      ],
      "metadata": {
        "id": "mg2w257VlNJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Directed: edges are not necessarily reciprocated\n",
        "\n",
        "Undirected: edges are always mutual\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.46.51.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.46.34.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.46.24.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.46.01.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.47.34.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.47.05.png)"
      ],
      "metadata": {
        "id": "3oFGtT4yfm2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unimodal vs. Multimodal"
      ],
      "metadata": {
        "id": "wYgsHp00lOlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unimodal networks (1-mode): include only one type of node (e.g. all nodes represent people)\n",
        "\n",
        "![](https://toreopsahl.files.wordpress.com/2009/04/fig1_twomode_simple.png)\n",
        "\n",
        "Multimodal (2-mode / Bipartite / Bimodal): include more than one type of node (.e.g people and research papers)\n",
        "\n",
        "![](https://toreopsahl.files.wordpress.com/2009/04/fig1_twomode_half.png)"
      ],
      "metadata": {
        "id": "u_4hJFTKgAH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unidimensional vs. Multidimensional"
      ],
      "metadata": {
        "id": "-8I_Q4NFlUz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unidimensional: includes one type of edges\n",
        "\n",
        "Multidimensional: Includes multiple types of edges (can be analysed as multiplex network or multiple distinct networks)\n",
        "\n",
        "Multidimensional networks are a special type of multilayer networks with multiple types of relations. They also consist of nodes and edges, but the nodes exist in separate layers, representing different forms of interactions, which connect to form an aspect. Aspects (or stacks of layers) can be used to represent different types of contacts, spatial locations, subsystems, or points in time\n",
        "\n",
        "Example:\n",
        "The multiplex social network of Star Wars saga. Each layer denotes a different episode and two nodes are connected to each other if the corresponding characters acted together in one or more scenes.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Muxviz_Star_Wars_Social_Network.png)"
      ],
      "metadata": {
        "id": "sjtZDb-CjItJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing networks"
      ],
      "metadata": {
        "id": "zHajRgMGhEq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Matrix plots"
      ],
      "metadata": {
        "id": "FsHCANjghJ9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.50.20.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.50.58.png)"
      ],
      "metadata": {
        "id": "b8ssFLOIhLuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Arc plots"
      ],
      "metadata": {
        "id": "IA4JS0bmhP20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.51.33.png)"
      ],
      "metadata": {
        "id": "mclxYej0hc8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Circos plots"
      ],
      "metadata": {
        "id": "9HgBCrAqhR7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.52.19.png)"
      ],
      "metadata": {
        "id": "tuaFUvFhhhmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Irrational vs. rational graphs"
      ],
      "metadata": {
        "id": "NVbel096g2tH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.49.42.png)"
      ],
      "metadata": {
        "id": "N8zfg0umg5Qe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NetworkX library"
      ],
      "metadata": {
        "id": "MAYJhQxWhSFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A graph object is a specific datastructure which contains node and edgelists jointly, and enables the application of graph algorithms on them. We work with the [`networkx`](https://networkx.github.io/documentation/stable/index.html) library, which is the standard for network analysis in the Python community."
      ],
      "metadata": {
        "id": "4RptyNgThUCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx # Main network analysis library"
      ],
      "metadata": {
        "id": "M6ILRfepxNU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In NetworkX, graph data are stored in a dictionary-like fashion.\n",
        "They are placed under a `Graph` object,\n",
        "canonically instantiated with the variable `G` as follows:\n",
        "\n",
        "```python\n",
        "G = nx.Graph()\n",
        "```\n",
        "\n",
        "Of course, you are free to name the graph anything you want!\n",
        "\n",
        "Nodes are part of the attribute `G.nodes`.\n",
        "There, the node data are housed in a dictionary-like container,\n",
        "where the key is the node itself\n",
        "and the values are a dictionary of attributes. \n",
        "Node data are accessible using syntax that looks like:\n",
        "\n",
        "```python\n",
        "G.nodes[node1]\n",
        "```\n",
        "\n",
        "Edges are part of the attribute `G.edges`,\n",
        "which is also stored in a dictionary-like container.\n",
        "Edge data are accessible using syntax that looks like: \n",
        "\n",
        "```python\n",
        "G.edges[node1, node2]\n",
        "```\n",
        "Because of the dictionary-like implementation of the graph,\n",
        "any hashable object can be a node.\n",
        "This means strings and tuples, but not lists and sets."
      ],
      "metadata": {
        "id": "aPCP1SrBRsIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local network structure (node-level measures)"
      ],
      "metadata": {
        "id": "arOixM0aUJxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methods to summarise the pattern of node connectivity to inter something on their characteristics.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.38.56.png)"
      ],
      "metadata": {
        "id": "cmW2pE-oUidz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Degree centrality"
      ],
      "metadata": {
        "id": "GkuGCdNAVB8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Counts the number of edges adjacent to a node.\n",
        "- Formally, the degree of node $i$ is the number of existing edges $e_{ij}$ with other nodes $j$ in a network with $n$ nodes:\n",
        "\n",
        "$$d_{ij} =\\sum\\limits_{j=1}^{n} e_{ij} ~ where: ~ i \\neq j$$\n",
        "\n",
        "**Degree centrality in directed networks**\n",
        "\n",
        "In directed networks, a node-pair has two different roles:\n",
        "\n",
        "* **Ego:** The node the edge originates from.\n",
        "* **Alter:** The node the edge leads to.\n",
        "\n",
        "Network metrics have to take directionality into account. For example, degree centrality is now differentiated between the\n",
        "- **in-degree** centrality (how many edges lead ***to*** the node)\n",
        "- **out-degree** centrality (how many edges lead ***from*** the node)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.43.47.png)"
      ],
      "metadata": {
        "id": "8ADMvslxVjvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eigenvector centrality"
      ],
      "metadata": {
        "id": "RHqmrFCFVXWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Weighs a node's degree centrality by the centrality of the nodes adjacent to it (and their centrality in turn by their centrality).\n",
        "\n",
        "$$x_{v}={\\frac {1}{\\lambda }}\\sum _{t\\in M(v)}x_{t}={\\frac {1}{\\lambda }}\\sum _{t\\in G}a_{v,t}x_{t}$$\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.48.08.png)"
      ],
      "metadata": {
        "id": "j360eHOeWLeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Betweenness centrality"
      ],
      "metadata": {
        "id": "oLp_3obCVc_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Measures the extent to which it lies on short paths.\n",
        "- A higher betweenness indicates that a node lies on more short paths and hence should somehow be important for traversing between different parts of a network.\n",
        "\n",
        "In formulaic representation\n",
        "\n",
        "* The geodesic betweenness $B_{n}(i)$ of a **vertex** in a weighted, undirected network is\n",
        "\n",
        "$$B_{n}(i) =  \\sum_{s,t \\in G} \\frac{ \\Psi_{s,t}(i) }{\\Psi_{s,t}}$$\n",
        "where vertices $s,t,i$ are all different from each other\n",
        "\n",
        "* $\\Psi_{s,t}$ denotes the number of shortest paths (geodesics) between vertices $s$ and $t$\n",
        "* $\\Psi_{s,t}(i)$ denotes the number of shortest paths (geodesics) between vertices $s$ and $t$ **that pass through vertex** $i$.\n",
        "* The geodesic betweenness $B_n$ of a network is the mean of $B_n(i)$ over all vertices $i$\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.51.47.png)"
      ],
      "metadata": {
        "id": "zjAhCzJFXSyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neighborhood"
      ],
      "metadata": {
        "id": "fGExOYbRYBDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Examines the surroundings of a node in terms of the nodes it is connected to, i.e. it's neighborhood\n",
        "- Ego-network of node: How many nodes are in a certain geodesic distance (meaning the shortest path), i.e. how many nodes are not more than x-steps away.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2012.57.16.png)"
      ],
      "metadata": {
        "id": "d0fHzek6YFwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UPDATE Modularity / Clustering (Community detection)\n",
        "what is within and between network connectivity??"
      ],
      "metadata": {
        "id": "op8IJNQoZISL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Group nodes based on graph topology (sometimes referred to as community detection based on its commonality in social network analysis)\n",
        "- Main logic: Form groups which have a ***maximum within-connectivity*** and a ***minimum between-connectivity***.\n",
        "- Consequently: Nodes in the same community should have a higher probability of being connected than nodes from different communities.\n",
        "\n",
        "**Community clustering in directed networks**\n",
        "\n",
        "Most community detection algorithms implemented in `NetworkX` only work with undirected networks. So, we can do 2 things to handle these:\n",
        "\n",
        "1. Convert the network in an undirected one.\n",
        "2. Use the \"edge betweenness\" algorithm, the only one implemented that can handle directed networks.\n",
        "\n",
        "There are (just like for clustering of tabular data in UML) many different algorithms and approaches to detect and delineate communities. [Here](https://github.com/benedekrozemberczki/awesome-community-detection) you find a summary of currently used approaches."
      ],
      "metadata": {
        "id": "sJBWbOoOZJkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: The Louvain Algorithm\n",
        "\n",
        "One of the most widely used community detection algorithms. It usually delivers good results, scales well, and can handle weighted networks. Furthermore, there is an actively maintained, easy to use Python implementation, [`python-louvain`](https://python-louvain.readthedocs.io).\n",
        "\n",
        "It optimises a quantity called modularity:\n",
        "\n",
        "$$  \\sum_{ij} (A_{ij} - \\lambda P_{ij}) \\delta(c_i,c_j) $$\n",
        "\n",
        "$A$ - The adjacency matrix\n",
        "\n",
        "$P_{ij}$ - The expected connection between $i$ and $j$.\n",
        "\n",
        "$\\lambda$ - Resolution parameter\n",
        "\n",
        "Can use lots of different forms for $P_{ij}$ but the standard one is the so called configuration model:\n",
        "\n",
        "$P_{ij} = \\frac{k_i k_j}{2m}$\n",
        "\n",
        "Loosely speaking, in an iterative process:\n",
        "- You take a node and try to aggregate it to one of its neighbours.\n",
        "- You choose the neighbour that maximizes a modularity function.\n",
        "- Once you iterate through all the nodes, you will have merged few nodes together and formed some communities.\n",
        "- This becomes the new input for the algorithm that will treat each community as a node and try to merge them together to create bigger communities.\n",
        "- The algorithm stops when it’s not possible to improve modularity any more.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2013.04.56.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2013.06.10.png)"
      ],
      "metadata": {
        "id": "E7OcvXIHaZRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assortiativity"
      ],
      "metadata": {
        "id": "N2qUHD_Fnrvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Measures if two nodes that share certain characteristics have a higher or lower probability to be connected.\n"
      ],
      "metadata": {
        "id": "WQgAKabQnuf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reciprocity\n",
        "\n",
        "- Measures if directed edges are reciptocated, meaning that an edge between `i,j` makes an edge between `j,i` more likely"
      ],
      "metadata": {
        "id": "AOt2P7_an5Qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global network structure (overall-level measures)"
      ],
      "metadata": {
        "id": "VvEAQMZtbml2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Density"
      ],
      "metadata": {
        "id": "UL41dMJEb5jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The density of a measure represents the share of all possible connections in the network."
      ],
      "metadata": {
        "id": "whjcF-_1cL6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transitivity / Clustering Coefficient"
      ],
      "metadata": {
        "id": "dVNbkMPzb-UK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transitivity, also called the Clustering Cefficient indicates how much the network tends to be locally clustered. That is measured by the share of closed triplets."
      ],
      "metadata": {
        "id": "Az01DuuCccMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diameter"
      ],
      "metadata": {
        "id": "ZGStDpQ0cBGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The diameter is the longest of the shortest paths between two nodes of the network."
      ],
      "metadata": {
        "id": "4bIojY_MdzrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean distance / Average path lenght"
      ],
      "metadata": {
        "id": "8UTh6SU3cDgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The mean distance / average path lenght represents the mean of all shortest paths between all nodes. It is a measure of diffusion potential within a network."
      ],
      "metadata": {
        "id": "ejUylNTbd9CR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Small worlds"
      ],
      "metadata": {
        "id": "RYx5dKfykeyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small worlds are an interesting network structure, combining short path lenght betwen the nodes with a high clustering coefficient. That means, that we have small interconected clusters, which are in turn connected by **gatekeepers** (the edges we call **bridges** or **structural holes**). \n",
        "\n",
        "A small-world network is a type of mathematical graph in which most nodes are not neighbors of one another, but the neighbors of any given node are likely to be neighbors of each other and most nodes can be reached from every other node by a small number of hops or steps.\n",
        "\n",
        "Mathematically, small world networks of size n have an average distance O(log n), meaning that between any two random nodes, the expected distance is O(log n).\n",
        "\n",
        "⟨L⟩ ∝ log n\n",
        "\n",
        "Small-world network example\n",
        "Hubs are bigger than other nodes\n",
        "Average degree= 3.833\n",
        "Average shortest path length = 1.803.\n",
        "Clustering coefficient = 0.522\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Small-world-network-example.png)"
      ],
      "metadata": {
        "id": "xRRqgXqlkgZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity networks"
      ],
      "metadata": {
        "id": "3Kppch4Zoz_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simirality networks are constructed by mapping similarity between all observations, e.g. using\n",
        "- Cosine Similarity\n",
        "- Pearson Coefficient\n",
        "- Euclidean Distance"
      ],
      "metadata": {
        "id": "NRWnf-Ico1tx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Cosine distance"
      ],
      "metadata": {
        "id": "tWLI1e1yvgod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity takes into account the degree of the vertices or how many common neighbors other pairs of vertices has and also allow for varying degrees of vertices.\n",
        "\n",
        "Salton proposed that we regard the i-th and j-th rows/columns of the adjacency matrix as two vectors and use the cosine of the angle between them as a similarity measure. The cosine similarity of i and j is the number of common neighbors divided by the geometric mean of their degrees.\n",
        "\n",
        "Its value lies in the range from 0 to 1. The value of 1 indicates that the two vertices have exactly the same neighbors while the value of zero means that they do not have any common neighbors. Cosine similarity is technically undefined if one or both of the nodes has zero degree, but according to the convention, we say that cosine similarity is 0 in these cases."
      ],
      "metadata": {
        "id": "ARgFFz7xvets"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pearson Coefficent"
      ],
      "metadata": {
        "id": "fjKeKQWEv45p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson product-moment correlation coefficient is an alternative method to normalize the count of common neighbors. This method compares the number of common neighbors with the expected value that count would take in a network where vertices are connected randomly. This quantity lies strictly in the range from -1 to 1."
      ],
      "metadata": {
        "id": "sTthL09Hv6yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Euclidean distance"
      ],
      "metadata": {
        "id": "rtZE1r4wwLoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Euclidean distance is equal to the number of neighbors that differ between two vertices. It is rather a dissimilarity measure, since it is larger for vertices which differ more. It could be normalized by dividing by its maximum value. The maximum means that there are no common neighbors, in which case the distance is equal to the sum of the degrees of the vertices."
      ],
      "metadata": {
        "id": "6ouKoBp4wFsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multimodal network analysis"
      ],
      "metadata": {
        "id": "uwCRKTh2pLiZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IhBLLexCOPh"
      },
      "source": [
        "Multi-modal means a network has several \"modes\", i.e. it connects entities on different conceptual levels. The most common is a **2-mode** (or **bipartite**) network. \n",
        "\n",
        "Examples could be:\n",
        "\n",
        "* Author $\\rightarrow$ Paper\n",
        "* Inventor $\\rightarrow$ Patent\n",
        "* Member $\\rightarrow$ Club network. \n",
        "\n",
        "Here, elements in different modes represent different things. In real-life research examples you find 2-mode networks in for instance:\n",
        "- co-occurence (2 actors mentioned in the same news-article)\n",
        "- co-affiliation (2 actors are member of the same association)\n",
        "- co-characteristics (2 actors both like to talk about a certain topic on twitter)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network projection"
      ],
      "metadata": {
        "id": "TRrhE8wEuFIH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rVL3iyWCOmY"
      },
      "source": [
        "Two-mode networks are rarely analysed in their original form. Although this is preferable, few methods exist for that purpose. As such, these networks are often transformed into one-mode networks (only one type of nodes) to be analysed. This procedure is often referred to as projection. Projection is done by selecting one of the sets of nodes and linking two nodes from that set if they were connected to the same node (of the other kind).\n",
        "\n",
        "We can alalyse them in sepperation (and sometimes we should), but often its helpful to *project* them onto one mode. Here, we create a node in one mode by joint association with another mode.\n",
        "\n",
        "2-mode\n",
        "\n",
        "![](https://toreopsahl.files.wordpress.com/2009/04/fig1_twomode_half.png)\n",
        "\n",
        "1-mode\n",
        "\n",
        "![](https://toreopsahl.files.wordpress.com/2009/04/fig1_twomode_simple.png)\n",
        "\n",
        "![](https://www.dropbox.com/s/e4vnq7kh24pyu0t/networks_2mode.png?dl=1)\n",
        "\n",
        "Particularly in citation networks, we can also use the implicite 2-mode structure of $Publications \\rightarrow Citation$\n",
        "\n",
        "That helps us to apply some interesting metrics, such as:\n",
        "\n",
        "* direct citations\n",
        "* Bibliographic coupling\n",
        "* Co--citations\n",
        "\n",
        "Interestingly, different projections of this 2-mode network give the whole resulting 1-mode network a different meaning.\n",
        "\n",
        "![](https://www.dropbox.com/s/f8g8nr83lucvpqx/networks_biblio.png?dl=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UPDATE proper projection"
      ],
      "metadata": {
        "id": "a7MaqCB8IO66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UPDATE quick and dirty projection\n",
        "\n",
        "Mergin the dataframe with itself based on one of the nodes as key, deleting selfloops and then you can create the edges."
      ],
      "metadata": {
        "id": "TOjQelLvILFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted network projection"
      ],
      "metadata": {
        "id": "Kr3b6TuguHpF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S92TTSjSr2jD"
      },
      "source": [
        "It is possible to discount for the number of nodes when projecting weighted two-mode networks.\n",
        " \n",
        " For example, it could be argued that if many online users post to a thread, their ties should be weaker than if there were few people posting to the thread. A straight forward generalisation is the following function: $w_{ij} = \\sum_p \\frac{w_{i,p}}{N_p - 1}$. \n",
        " \n",
        " This formula would create a directed one-mode network in which the out-strength of a node is equal to the sum of the weights attached to the ties in the two-mode network that originated from that node. For example, node C has a tie with a weight of 5 in the two-mode network and an out-strength of 5 in the one-mode projection.\n",
        "\n",
        "![](https://toreopsahl.files.wordpress.com/2009/04/fig1_twomode_forum_newman2001.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing"
      ],
      "metadata": {
        "id": "GClPkVQAtIP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elements of text vs. Text as such"
      ],
      "metadata": {
        "id": "THaFEZ3w74Sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interest of analysis goes into the direction of: Either were analysing the text as such (what is the statement in the text) or elements of text.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-11-06%20at%2013.31.59.png)\n"
      ],
      "metadata": {
        "id": "8AELOs5KtQqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing concepts"
      ],
      "metadata": {
        "id": "u4fpxowcJEMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntax and semantics. The syntax is the grammatical structure of the text, and semantics is the meaning being conveyed.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes//main/Images/vURoVPyiqdRaOmec.png)"
      ],
      "metadata": {
        "id": "0sE_GfUjRo1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "q0Al0q-4RiVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokenization separates a piece of text into smaller units called tokens\n",
        "- Tokenization can be broadly classified into 3 types:\n",
        "  - word tokens: smarter\n",
        "  - character tokens: s-m-a-r-t-e-r\n",
        "  - subword token (n-gram characters): smart-er\n",
        "- But different methods may be applied when tokenizing\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/tokenize.png)"
      ],
      "metadata": {
        "id": "Vf8UDeJtRtEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization"
      ],
      "metadata": {
        "id": "yaxqxgQQSgQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Reduces randomness of text, bringing it closer to a predefined 'standard, i.e. improving efficiency\n",
        "- Before normalizing:\n",
        "  - expand contractions (by making a dictionary)\n",
        "  - tokenize\n",
        "  - remove punctuations\n",
        "- Normalization techniques:\n",
        "  - Stemming: reducing words to their word stem or root form (may not be a dictionary word)\n",
        "    - Over-stemming: more than required is removed, e.g. “university” and “universe” = “univers”.\n",
        "    - Under-stemming: less than required is removed, e.g. “data” and “datum” = “dat” and “datu” (instead of “dat”).\n",
        "  - Lemmatization: reducing words to their base word in the language (is in the dictionary)\n",
        "    - A root word is called lemma. A lemma is the canonical form, dictionary form, or citation form of a set of words.\n",
        "\n",
        "\n",
        "Should you always normalize?\n",
        "\n",
        "No, it depends on the problem.\n",
        "- Lemmatization is needed for topic modeling and for training word vectors, which is dependent of string matches and accurate word counts\n",
        "- Semantic analysis methods on the other hand have different ratings depending on the form of the word and therefore the input should not be stemmed or lemmatized.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/pLfTViDgRXuHYvWE-2.png)"
      ],
      "metadata": {
        "id": "Lr_YIpbGSh0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequency based dictionary filtering"
      ],
      "metadata": {
        "id": "zk0uFyYNSj3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Removing stop words, i.e. filtering out high-frequency words that add little or no semantic value to a sentence, e.g.:\n",
        "  - to, for, on, and, the, etc.\n",
        "  - You can even create custom lists of stopwords to include words that you want to ignore."
      ],
      "metadata": {
        "id": "y4tH7-ljSmLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS tagging (Part-of-Speech) (TAGGER)"
      ],
      "metadata": {
        "id": "MDYZRLBQSpu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Assigning each word (token) in a sentence the part of speech category that it assumes in that sentence.\n",
        "- Target is to identify the grammatical group of a given word: noun, pronoun, adjective, verb, adverbs, etc. based on the context.\n",
        "- POS tagging improves accuracy, .e.g ‘leaves’ without a POS tag would get lemmatized to ‘leaf’, but with a verb tag, its lemma would be ‘leave’.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/GVCinaTpbVEEOukr.png)"
      ],
      "metadata": {
        "id": "qWkuy6g2Sshl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependency Parsing (PARSER)"
      ],
      "metadata": {
        "id": "wHhn4eeESvBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency grammar refers to the way the words in a sentence are connected. A dependency parser, therefore, analyzes how ‘head words’ are related and modified by other words to understand the syntactic structure of a sentence.\n",
        "\n",
        "nsubj: nominal subject\n",
        "\n",
        "dobj: direct object\n",
        "\n",
        "conj: conjugation\n",
        "\n",
        "advmod: adverbial modifier\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/aEFHscfTPxihutra.png)"
      ],
      "metadata": {
        "id": "pwPLD5l9Sw2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "q7sIdF09S0-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Extracts entities from text documents, e.g. names, places, organizations, email addresses, etc.\n",
        "- Relationship extraction takes this one step further and finds relationships between two nouns. e.g. “Lumiere lives in Nice,” a person (Lumiere) is related to a place (Nice) by the semantic category “lives in.”\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/AVySmceRpFKFjcaM.png)\n",
        "\n",
        "RJ example: From text to network\n",
        "\n",
        "There may be relationships between entitites in the text which can be translated to represent relationships with network structures. It may even be directed relationships\n",
        "\n",
        "How to get there:\n",
        "\n",
        "Look at grammar. What are the subjects, objects, and how are they related. Packages exist for this, turning texts into elements and performing analysis on the elements. I.e. representing text as a relational structure, creating an edgelist, nodelist, etc.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-11-06%20at%2014.02.49.png)"
      ],
      "metadata": {
        "id": "0WgZCHEYLPvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization approaches"
      ],
      "metadata": {
        "id": "MNMrgP_HJNUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Container model (not vectorization, intro from RJ videos)"
      ],
      "metadata": {
        "id": "cVhYCZi6arPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario: we have a sentence 'This is a very nice house'\n",
        "\n",
        "Think about the sentence as a container - it contains words. I.e. it's a way to represent meaning through combinations of elements.\n",
        "\n",
        "If the sentence is a container of terms, the terms carry meaning and the combination of terms is the meaning of the sentence. I.e. some terms bear more meaning (debatable) e.g. house and nice are important, whereas very, but, this, is, a and . may not be that important.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-11-06%20at%2013.58.00.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bUz4HG0OarPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of words model (BoW) (Bag of words representation of text)"
      ],
      "metadata": {
        "id": "vDodESNHarPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to apply ML approaches, we need tabular data including text and features.\n",
        "- text = each sentence\n",
        "- features = f1 to fn is the vocabulary contained in the whole collection of texts, i.e. all words used in the sentences\n",
        "\n",
        "Example:\n",
        "\n",
        "the fat cat sits on the mat\n",
        "\n",
        "the cat ate a fat rat\n",
        "\n",
        "the dog is sad\n",
        "\n",
        "Procedure:\n",
        "- Kick out words that don't carry much meaning\n",
        "  \n",
        "  - e.g. articles the, is, a etc. preprositions and full stops.\n",
        "\n",
        "- Normalize text, stemming or lemmatizing\n",
        "  - Normalizing the vocab by removing gender, declension and conjugation. Changes the verbs according to whether is plural, singular, etc. and transform past tense into present.\n",
        "  - Reason for normalizing: Discovering meaning is the same in normalized text, as long as what you want to figure out is related to meaning and not so much to gender or time etc. (in that case do not normliaze)\n",
        "\n",
        "- Turn the vocab into a table\n",
        "  - Rows: Text (sentences)\n",
        "  - Columns: Features (words)\n",
        "\n",
        "- Add 1's and 0's\n",
        "  - depending if the word is present in the sentence or not. Result is a sparse matrix representing text in tabular form\n",
        "\n",
        "- Apply ML models\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-29%20at%2016.51.35.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2007.56.59.png)"
      ],
      "metadata": {
        "id": "Sg7UEn2warPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UPDATE Embeddings??"
      ],
      "metadata": {
        "id": "nVEdUk23bNTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "nEz4vpwxOYS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In simple terms:\n",
        "- The order of things in a sentence, e.g. dog bites man vs man bites dog\n",
        "\n",
        "Word2Vec = word embeddings\n",
        "\n",
        "Rational/mechanism behind word2vec:\n",
        "- The distributional hypopothesis \"You shall know a word by the company it keeps\", i.e. the meaning of a word is defined by the context in which it tends to occur. By observing the words surrounding a word will give us an understanding of what the word means.\n",
        "\n",
        "The word2vec algorithm is a formalization of that. You look at the first three words and try to predict the 4th word. Once you're done with that you move the window by one and try to predict word 5 etc. etc.\n",
        "\n",
        "Imagine you're taking all your words in a text or corpus, creating a SML classification problem where input is x, the first 3 words, and y is word number 4 etc. etc. etc.\n",
        "\n",
        "The model is a shallow naural network. What happens is:\n",
        "- By running above again and again you identify patterns that map the relationship between the terms and all possible contexts it has. Ending up with a vector representation (word embedding) of the words that carry the meaning of the word. Once you have these word embeddings you can figure out the following:\n",
        "- Similar terms have similar vectors, because similar terms tend to appear in similar contexts, e.g. the word cat will appear in similar contexts as feline\n",
        "- Vberlin - Vparis + Vbeer = Vwine, it captures not only the words but also the context and allows you to do linear algebra on the words and find the latent structures that are underlying.\n",
        "- Average vector of all the words in a sentence as a document representation\n",
        "- Input for deep learning applications\n",
        "  - What if were not only looking at words in a sentence but feeding the model with vectors that represent the words in the right order. Feeding word vectors into a neural network into specific models, going deeper into meaning structures.\n",
        "\n",
        "Run the exercise on a LOT of text and train the model and you will end up with something thats really good at finding the meaning of words.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2008.36.37.png)\n",
        "\n",
        "The Word2vec model captures both syntactic and semantic similarities between the words. One of the well known examples of the vector algebraic on the trained word2vec vectors is:\n",
        "\n",
        "Vector(“King”)-Vector(“Man”)= Vector(“Queen”)-Vector(“Woman)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/1*5F4TXdFYwqi-BWTToQPIfg.jpeg)"
      ],
      "metadata": {
        "id": "ZFDnflk6xkUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF (Term Frequency - Inverse Document Frequency)"
      ],
      "metadata": {
        "id": "2qyE_ssearPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF tries to weigh words, i.e. if a word appears in all texts it may not be as important for the individual texts. Original words are probably more important than general words. e.g. the word 'president' in political tweets.\n",
        "\n",
        "The weight of term x and sentence y is equal to the term frequency of the word x in sentence y times the logarithm of the total number of documents divided by dfx (the number of documents the word x appears in, i.e. cannot be larger than N).\n",
        "\n",
        "The closer log is to one, the lower the importance of the word\n",
        "\n",
        "i.e. TF-IDF discount general terms / highlight specific terms. In reality just looking a frequency distribution.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2008.01.35.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "ymjA6MrzarPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality reduction and topic modeling\n",
        "\n",
        "LSA\n",
        "SVD\n",
        "LDA"
      ],
      "metadata": {
        "id": "qYiVPvZdJsFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From Bag of words (BoW) to topic modeling and embeddings"
      ],
      "metadata": {
        "id": "d0XlQqF3arPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SML:\n",
        "- BoW and TF-IDF is sufficient for many SML approaches, e.g. classification or sentiment analysis. Performance can be quite alright with BoW but better with TF-IDF\n",
        "\n",
        "Going from sparse matrix to dense:\n",
        "- LSI (latent semantic indexing) or LSA (latent semantic analysis).\n",
        "- Similar to SVD (singular value decomposition) or NMF (non negative matrix) in terms of mathematics and output\n",
        "\n",
        "Simplified example using LSI/LSA:\n",
        "- We have a sparse matrix with words and docs\n",
        "- Transforming the matrix to dense using dimensionality reduction approaches (LSI or LSA)\n",
        "- The result is one matrix with documents vs. topics and one matrix with topics vs. vocab\n",
        "  - documents vs. topics: dense matrix that identifies topics, similar to UML, i.e. components. Topics are easy to interpret because the way we speak and write is logical. Linear algebra can help uncover latent topics in text corpora. Each document is represented as a combination of topics.\n",
        "  - topics vs. vocab: dense matrix that look at to which extent terms contribute to topics. What is the relationship between certain terms and certain topics. This is a lower dimension matrix that can be used for:\n",
        "      - SML\n",
        "      - Similarity (e.g. cosine similarity. The advantage of similarity based on reduced matrices compared to BoW/TF-IDF is that you are able to get similarity of two documents that don't share any words but contains certain synonyms. With BoW you need to have exact common terms, LSI/LSA on the other hand is based on topics and not words, i.e. getting closer to semantic similarity.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/BDS_M2_Exam_Notes/main/Images/Screenshot%202022-10-30%20at%2008.14.36.png)\n",
        "\n",
        "\n",
        "LDA (latent dirichlet allocation):\n",
        "- Another topic modeling approach\n",
        "- Probabilistc approach\n",
        "- Wouldn't use it for SML, but great for topic discovery and visualizations\n",
        "\n",
        "LSI (also known as Latent Semantic Analysis, LSA) learns latent topics by performing a matrix decomposition (SVD) on the term-document matrix.\n",
        "\n",
        "LDA is a generative probabilistic model, that assumes a Dirichlet prior over the latent topics.\n",
        "\n",
        "In practice, LSI is much faster to train than LDA, but has lower accuracy.\n",
        "\n",
        "Suppose you have the following set of sentences:\n",
        "\n",
        "I ate a banana and spinach smoothie for breakfast\n",
        "I like to eat broccoli and bananas.\n",
        "Chinchillas and kittens are cute.\n",
        "My sister adopted a kitten yesterday.\n",
        "Look at this cute hamster munching on a piece of broccoli.\n",
        "\n",
        "Latent Dirichlet allocation is a way of automatically discovering topics that these sentences contain. For example, given these sentences and asked for 2 topics, LDA might produce something like\n",
        "\n",
        "Sentences 1 and 2: 100% Topic A\n",
        "Sentences 3 and 4: 100% Topic B\n",
        "Sentence 5: 60% Topic A, 40% Topic B\n",
        "Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, ... (at which point, you could interpret topic A to be about food)\n",
        "Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, ... (at which point, you could interpret topic B to be about cute animals)\n",
        "\n",
        "\n",
        "Corex:\n",
        "- Can we make existing approaches more informative\n",
        "- Anchor words - means that you get some kind of semi-supervised  representation. So you can infuse the model with some domain expertise. E.g. I know there are some topics in this corpus and I know these are terms that belong to the different topics, so take these into account when you are making the groupings automatically\n",
        "\n"
      ],
      "metadata": {
        "id": "BnNaGt5FarPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP in supervised pipelines"
      ],
      "metadata": {
        "id": "v3dSCMhgJzsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explainability"
      ],
      "metadata": {
        "id": "cLXhwzdTJ1_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Typical applications"
      ],
      "metadata": {
        "id": "8r0f2fWMaB8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Industry Applications Of NLP\n",
        "News Aggregation\n",
        "Social Media Monitoring\n",
        "Analytics For Marketing\n",
        "Automated Customer Service\n",
        "Content Filtering- Spam filters\n",
        "\n",
        "semantic search\n",
        "\n",
        "Text Classification\n",
        "\n",
        "Text Classification Illustration\n",
        "Text classification is the process of understanding the meaning of the unstructured text and organizing it into predefined classes (also called labels, or tags). One of the most popular text classification tasks is sentiment analysis, which aims to categorize unstructured text data by sentiment. Other common classification tasks include intent detection, topic modeling, and language detection.\n",
        "\n",
        "\n",
        "\n",
        "chatbots\n",
        "\n",
        "Approach:\n",
        "- Train a model on variants of a question.\n",
        "- Take input and predict the type of question asked - this is called “intent”\n",
        "- Reply with a pre-defined response corresponding to the question asked.\n",
        "\n",
        "\n",
        "Modern bots are more complex. They evaluate the whole (or large parts of the) dialogue. In addition some have the capacity to generate text.\n",
        "\n",
        "The data needed for building such a system is a collection from a company FAQ for instance with variations for each question-answer pair type. For examples with many q-a pairs like the banking-faq, you can consider similarity-based approaches, where input text is matched with most \"similar\" predifined questions and then the user picks upon request - \"did you mean xyz-question\" Alternatively, one could create paraphrased versions of questions for each question-answer pair to have more training examples. This can be done manually or\n",
        "The overall architecture is as follows:\n",
        "Given a free text/prompt, predict which question is asked (intent)\n",
        "Pick corresponding answer (e.g. random out of 2-3) to simulate dialogue\n",
        "In this notebook, we will first use TFIDF-Logit, then standard SpaCy vectors and finally Floret (new SpaCy vectors that combine a new efficient compression with fastText, that helps overcome typos by including subword-elements in the model)"
      ],
      "metadata": {
        "id": "pME_8M70J45V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word vectors (intuition)"
      ],
      "metadata": {
        "id": "np00s_NcJ8Jd"
      }
    }
  ]
}